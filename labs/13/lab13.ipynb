{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13 - Ridge Regression\n",
    "# Lecture 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted = readRDS('data/ted/ted_talks.rds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>2550</li><li>452</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2550\n",
       "\\item 452\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2550\n",
       "2. 452\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2550  452"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(ted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 452</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>duration</th><th scope=col>film_date</th><th scope=col>num_speaker</th><th scope=col>published_date</th><th scope=col>views</th><th scope=col>wordfreq_children</th><th scope=col>wordfreq_creativity</th><th scope=col>wordfreq_culture</th><th scope=col>wordfreq_dance</th><th scope=col>wordfreq_education</th><th scope=col>...</th><th scope=col>ratingfreq_Longwinded</th><th scope=col>ratingfreq_Confusing</th><th scope=col>ratingfreq_Informative</th><th scope=col>ratingfreq_Fascinating</th><th scope=col>ratingfreq_Unconvincing</th><th scope=col>ratingfreq_Persuasive</th><th scope=col>ratingfreq_Jaw.dropping</th><th scope=col>ratingfreq_OK</th><th scope=col>ratingfreq_Obnoxious</th><th scope=col>ratingfreq_Inspiring</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>...</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1164</td><td>1140825600</td><td>1</td><td>1151367060</td><td>47227110</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>...</td><td>0.004123601</td><td>0.002578583</td><td>0.078273841</td><td>0.11274374</td><td>0.00319659</td><td>0.114054342</td><td>0.04729888</td><td>0.01250932</td><td>0.002226958</td><td>0.265572722</td></tr>\n",
       "\t<tr><th scope=row>2</th><td> 977</td><td>1140825600</td><td>1</td><td>1151367060</td><td> 3200520</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>...</td><td>0.087874659</td><td>0.038487738</td><td>0.150885559</td><td>0.14066757</td><td>0.04495913</td><td>0.019073569</td><td>0.09128065</td><td>0.03950954</td><td>0.044618529</td><td>0.069141689</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>1286</td><td>1140739200</td><td>1</td><td>1151367060</td><td> 1636292</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0.036827195</td><td>0.027620397</td><td>0.139872521</td><td>0.08144476</td><td>0.05878187</td><td>0.009560907</td><td>0.05169972</td><td>0.08144476</td><td>0.019121813</td><td>0.050283286</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1116</td><td>1140912000</td><td>1</td><td>1151367060</td><td> 1697550</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0.028165236</td><td>0.009656652</td><td>0.014216738</td><td>0.10193133</td><td>0.28701717</td><td>0.035407725</td><td>0.12339056</td><td>0.06169528</td><td>0.009388412</td><td>0.022800429</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>1190</td><td>1140566400</td><td>1</td><td>1151440680</td><td>12005869</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0.004293521</td><td>0.002810304</td><td>0.002615144</td><td>0.21206089</td><td>0.11291959</td><td>0.179781421</td><td>0.14582358</td><td>0.09921936</td><td>0.009679938</td><td>0.002380952</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>1305</td><td>1138838400</td><td>1</td><td>1151440680</td><td>20685401</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>...</td><td>0.045975514</td><td>0.025853087</td><td>0.018559521</td><td>0.06759573</td><td>0.33934618</td><td>0.087913519</td><td>0.15778849</td><td>0.02871842</td><td>0.043566033</td><td>0.021815577</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 452\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & duration & film\\_date & num\\_speaker & published\\_date & views & wordfreq\\_children & wordfreq\\_creativity & wordfreq\\_culture & wordfreq\\_dance & wordfreq\\_education & ... & ratingfreq\\_Longwinded & ratingfreq\\_Confusing & ratingfreq\\_Informative & ratingfreq\\_Fascinating & ratingfreq\\_Unconvincing & ratingfreq\\_Persuasive & ratingfreq\\_Jaw.dropping & ratingfreq\\_OK & ratingfreq\\_Obnoxious & ratingfreq\\_Inspiring\\\\\n",
       "  & <int> & <int> & <int> & <int> & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ... & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 1164 & 1140825600 & 1 & 1151367060 & 47227110 & 1 & 1 & 1 & 1 & 1 & ... & 0.004123601 & 0.002578583 & 0.078273841 & 0.11274374 & 0.00319659 & 0.114054342 & 0.04729888 & 0.01250932 & 0.002226958 & 0.265572722\\\\\n",
       "\t2 &  977 & 1140825600 & 1 & 1151367060 &  3200520 & 0 & 0 & 1 & 0 & 0 & ... & 0.087874659 & 0.038487738 & 0.150885559 & 0.14066757 & 0.04495913 & 0.019073569 & 0.09128065 & 0.03950954 & 0.044618529 & 0.069141689\\\\\n",
       "\t3 & 1286 & 1140739200 & 1 & 1151367060 &  1636292 & 0 & 0 & 0 & 0 & 0 & ... & 0.036827195 & 0.027620397 & 0.139872521 & 0.08144476 & 0.05878187 & 0.009560907 & 0.05169972 & 0.08144476 & 0.019121813 & 0.050283286\\\\\n",
       "\t4 & 1116 & 1140912000 & 1 & 1151367060 &  1697550 & 0 & 0 & 0 & 0 & 0 & ... & 0.028165236 & 0.009656652 & 0.014216738 & 0.10193133 & 0.28701717 & 0.035407725 & 0.12339056 & 0.06169528 & 0.009388412 & 0.022800429\\\\\n",
       "\t5 & 1190 & 1140566400 & 1 & 1151440680 & 12005869 & 0 & 0 & 0 & 0 & 0 & ... & 0.004293521 & 0.002810304 & 0.002615144 & 0.21206089 & 0.11291959 & 0.179781421 & 0.14582358 & 0.09921936 & 0.009679938 & 0.002380952\\\\\n",
       "\t6 & 1305 & 1138838400 & 1 & 1151440680 & 20685401 & 0 & 0 & 1 & 0 & 0 & ... & 0.045975514 & 0.025853087 & 0.018559521 & 0.06759573 & 0.33934618 & 0.087913519 & 0.15778849 & 0.02871842 & 0.043566033 & 0.021815577\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 452\n",
       "\n",
       "| <!--/--> | duration &lt;int&gt; | film_date &lt;int&gt; | num_speaker &lt;int&gt; | published_date &lt;int&gt; | views &lt;int&gt; | wordfreq_children &lt;dbl&gt; | wordfreq_creativity &lt;dbl&gt; | wordfreq_culture &lt;dbl&gt; | wordfreq_dance &lt;dbl&gt; | wordfreq_education &lt;dbl&gt; | ... ... | ratingfreq_Longwinded &lt;dbl&gt; | ratingfreq_Confusing &lt;dbl&gt; | ratingfreq_Informative &lt;dbl&gt; | ratingfreq_Fascinating &lt;dbl&gt; | ratingfreq_Unconvincing &lt;dbl&gt; | ratingfreq_Persuasive &lt;dbl&gt; | ratingfreq_Jaw.dropping &lt;dbl&gt; | ratingfreq_OK &lt;dbl&gt; | ratingfreq_Obnoxious &lt;dbl&gt; | ratingfreq_Inspiring &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 1164 | 1140825600 | 1 | 1151367060 | 47227110 | 1 | 1 | 1 | 1 | 1 | ... | 0.004123601 | 0.002578583 | 0.078273841 | 0.11274374 | 0.00319659 | 0.114054342 | 0.04729888 | 0.01250932 | 0.002226958 | 0.265572722 |\n",
       "| 2 |  977 | 1140825600 | 1 | 1151367060 |  3200520 | 0 | 0 | 1 | 0 | 0 | ... | 0.087874659 | 0.038487738 | 0.150885559 | 0.14066757 | 0.04495913 | 0.019073569 | 0.09128065 | 0.03950954 | 0.044618529 | 0.069141689 |\n",
       "| 3 | 1286 | 1140739200 | 1 | 1151367060 |  1636292 | 0 | 0 | 0 | 0 | 0 | ... | 0.036827195 | 0.027620397 | 0.139872521 | 0.08144476 | 0.05878187 | 0.009560907 | 0.05169972 | 0.08144476 | 0.019121813 | 0.050283286 |\n",
       "| 4 | 1116 | 1140912000 | 1 | 1151367060 |  1697550 | 0 | 0 | 0 | 0 | 0 | ... | 0.028165236 | 0.009656652 | 0.014216738 | 0.10193133 | 0.28701717 | 0.035407725 | 0.12339056 | 0.06169528 | 0.009388412 | 0.022800429 |\n",
       "| 5 | 1190 | 1140566400 | 1 | 1151440680 | 12005869 | 0 | 0 | 0 | 0 | 0 | ... | 0.004293521 | 0.002810304 | 0.002615144 | 0.21206089 | 0.11291959 | 0.179781421 | 0.14582358 | 0.09921936 | 0.009679938 | 0.002380952 |\n",
       "| 6 | 1305 | 1138838400 | 1 | 1151440680 | 20685401 | 0 | 0 | 1 | 0 | 0 | ... | 0.045975514 | 0.025853087 | 0.018559521 | 0.06759573 | 0.33934618 | 0.087913519 | 0.15778849 | 0.02871842 | 0.043566033 | 0.021815577 |\n",
       "\n"
      ],
      "text/plain": [
       "  duration film_date  num_speaker published_date views    wordfreq_children\n",
       "1 1164     1140825600 1           1151367060     47227110 1                \n",
       "2  977     1140825600 1           1151367060      3200520 0                \n",
       "3 1286     1140739200 1           1151367060      1636292 0                \n",
       "4 1116     1140912000 1           1151367060      1697550 0                \n",
       "5 1190     1140566400 1           1151440680     12005869 0                \n",
       "6 1305     1138838400 1           1151440680     20685401 0                \n",
       "  wordfreq_creativity wordfreq_culture wordfreq_dance wordfreq_education ...\n",
       "1 1                   1                1              1                  ...\n",
       "2 0                   1                0              0                  ...\n",
       "3 0                   0                0              0                  ...\n",
       "4 0                   0                0              0                  ...\n",
       "5 0                   0                0              0                  ...\n",
       "6 0                   1                0              0                  ...\n",
       "  ratingfreq_Longwinded ratingfreq_Confusing ratingfreq_Informative\n",
       "1 0.004123601           0.002578583          0.078273841           \n",
       "2 0.087874659           0.038487738          0.150885559           \n",
       "3 0.036827195           0.027620397          0.139872521           \n",
       "4 0.028165236           0.009656652          0.014216738           \n",
       "5 0.004293521           0.002810304          0.002615144           \n",
       "6 0.045975514           0.025853087          0.018559521           \n",
       "  ratingfreq_Fascinating ratingfreq_Unconvincing ratingfreq_Persuasive\n",
       "1 0.11274374             0.00319659              0.114054342          \n",
       "2 0.14066757             0.04495913              0.019073569          \n",
       "3 0.08144476             0.05878187              0.009560907          \n",
       "4 0.10193133             0.28701717              0.035407725          \n",
       "5 0.21206089             0.11291959              0.179781421          \n",
       "6 0.06759573             0.33934618              0.087913519          \n",
       "  ratingfreq_Jaw.dropping ratingfreq_OK ratingfreq_Obnoxious\n",
       "1 0.04729888              0.01250932    0.002226958         \n",
       "2 0.09128065              0.03950954    0.044618529         \n",
       "3 0.05169972              0.08144476    0.019121813         \n",
       "4 0.12339056              0.06169528    0.009388412         \n",
       "5 0.14582358              0.09921936    0.009679938         \n",
       "6 0.15778849              0.02871842    0.043566033         \n",
       "  ratingfreq_Inspiring\n",
       "1 0.265572722         \n",
       "2 0.069141689         \n",
       "3 0.050283286         \n",
       "4 0.022800429         \n",
       "5 0.002380952         \n",
       "6 0.021815577         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(ted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = ted[,grep(\"word\",colnames(ted),value=TRUE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'wordfreq_children'</li><li>'wordfreq_creativity'</li><li>'wordfreq_culture'</li><li>'wordfreq_dance'</li><li>'wordfreq_education'</li><li>'wordfreq_parenting'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'wordfreq\\_children'\n",
       "\\item 'wordfreq\\_creativity'\n",
       "\\item 'wordfreq\\_culture'\n",
       "\\item 'wordfreq\\_dance'\n",
       "\\item 'wordfreq\\_education'\n",
       "\\item 'wordfreq\\_parenting'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'wordfreq_children'\n",
       "2. 'wordfreq_creativity'\n",
       "3. 'wordfreq_culture'\n",
       "4. 'wordfreq_dance'\n",
       "5. 'wordfreq_education'\n",
       "6. 'wordfreq_parenting'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"wordfreq_children\"   \"wordfreq_creativity\" \"wordfreq_culture\"   \n",
       "[4] \"wordfreq_dance\"      \"wordfreq_education\"  \"wordfreq_parenting\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(colnames(tdm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 434</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>views</th><th scope=col>wordfreq_children</th><th scope=col>wordfreq_creativity</th><th scope=col>wordfreq_culture</th><th scope=col>wordfreq_dance</th><th scope=col>wordfreq_education</th><th scope=col>wordfreq_parenting</th><th scope=col>wordfreq_teaching</th><th scope=col>wordfreq_alternative</th><th scope=col>wordfreq_cars</th><th scope=col>...</th><th scope=col>wordfreq_capitalism</th><th scope=col>wordfreq_refugees</th><th scope=col>wordfreq_grammar</th><th scope=col>wordfreq_discovery</th><th scope=col>wordfreq_blockchain</th><th scope=col>wordfreq_tednyc</th><th scope=col>wordfreq_residency</th><th scope=col>wordfreq_biosphere</th><th scope=col>wordfreq_epidemiology</th><th scope=col>wordfreq_funny</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>...</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>47227110</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>2</th><td> 3200520</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>3</th><td> 1636292</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>4</th><td> 1697550</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>12005869</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>20685401</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 434\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & views & wordfreq\\_children & wordfreq\\_creativity & wordfreq\\_culture & wordfreq\\_dance & wordfreq\\_education & wordfreq\\_parenting & wordfreq\\_teaching & wordfreq\\_alternative & wordfreq\\_cars & ... & wordfreq\\_capitalism & wordfreq\\_refugees & wordfreq\\_grammar & wordfreq\\_discovery & wordfreq\\_blockchain & wordfreq\\_tednyc & wordfreq\\_residency & wordfreq\\_biosphere & wordfreq\\_epidemiology & wordfreq\\_funny\\\\\n",
       "  & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ... & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 47227110 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t2 &  3200520 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t3 &  1636292 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t4 &  1697550 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t5 & 12005869 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t6 & 20685401 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 434\n",
       "\n",
       "| <!--/--> | views &lt;int&gt; | wordfreq_children &lt;dbl&gt; | wordfreq_creativity &lt;dbl&gt; | wordfreq_culture &lt;dbl&gt; | wordfreq_dance &lt;dbl&gt; | wordfreq_education &lt;dbl&gt; | wordfreq_parenting &lt;dbl&gt; | wordfreq_teaching &lt;dbl&gt; | wordfreq_alternative &lt;dbl&gt; | wordfreq_cars &lt;dbl&gt; | ... ... | wordfreq_capitalism &lt;dbl&gt; | wordfreq_refugees &lt;dbl&gt; | wordfreq_grammar &lt;dbl&gt; | wordfreq_discovery &lt;dbl&gt; | wordfreq_blockchain &lt;dbl&gt; | wordfreq_tednyc &lt;dbl&gt; | wordfreq_residency &lt;dbl&gt; | wordfreq_biosphere &lt;dbl&gt; | wordfreq_epidemiology &lt;dbl&gt; | wordfreq_funny &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 47227110 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| 2 |  3200520 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| 3 |  1636292 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| 4 |  1697550 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| 5 | 12005869 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| 6 | 20685401 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  views    wordfreq_children wordfreq_creativity wordfreq_culture\n",
       "1 47227110 1                 1                   1               \n",
       "2  3200520 0                 0                   1               \n",
       "3  1636292 0                 0                   0               \n",
       "4  1697550 0                 0                   0               \n",
       "5 12005869 0                 0                   0               \n",
       "6 20685401 0                 0                   1               \n",
       "  wordfreq_dance wordfreq_education wordfreq_parenting wordfreq_teaching\n",
       "1 1              1                  1                  1                \n",
       "2 0              0                  0                  0                \n",
       "3 0              0                  0                  0                \n",
       "4 0              0                  0                  0                \n",
       "5 0              0                  0                  0                \n",
       "6 0              0                  0                  0                \n",
       "  wordfreq_alternative wordfreq_cars ... wordfreq_capitalism wordfreq_refugees\n",
       "1 0                    0             ... 0                   0                \n",
       "2 1                    1             ... 0                   0                \n",
       "3 0                    0             ... 0                   0                \n",
       "4 0                    0             ... 0                   0                \n",
       "5 0                    0             ... 0                   0                \n",
       "6 0                    0             ... 0                   0                \n",
       "  wordfreq_grammar wordfreq_discovery wordfreq_blockchain wordfreq_tednyc\n",
       "1 0                0                  0                   0              \n",
       "2 0                0                  0                   0              \n",
       "3 0                0                  0                   0              \n",
       "4 0                0                  0                   0              \n",
       "5 0                0                  0                   0              \n",
       "6 0                0                  0                   0              \n",
       "  wordfreq_residency wordfreq_biosphere wordfreq_epidemiology wordfreq_funny\n",
       "1 0                  0                  0                     0             \n",
       "2 0                  0                  0                     0             \n",
       "3 0                  0                  0                     0             \n",
       "4 0                  0                  0                     0             \n",
       "5 0                  0                  0                     0             \n",
       "6 0                  0                  0                     0             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = data.frame(views=ted$views,tdm)\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = lm(log(views)~.,data=df)\n",
    "beta_hat = mod$coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "21"
      ],
      "text/latex": [
       "21"
      ],
      "text/markdown": [
       "21"
      ],
      "text/plain": [
       "[1] 21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(!is.finite(beta_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "412"
      ],
      "text/latex": [
       "412"
      ],
      "text/markdown": [
       "412"
      ],
      "text/plain": [
       "[1] 412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qr(tdm)$rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>2550</li><li>433</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2550\n",
       "\\item 433\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2550\n",
       "2. 433\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2550  433"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>3.81922858801525e-15</li><li>3.81922858801525e-15</li><li>3.48552307237904e-15</li><li>2.78050117475054e-15</li><li>2.29611674802203e-15</li><li>9.86054878339099e-16</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3.81922858801525e-15\n",
       "\\item 3.81922858801525e-15\n",
       "\\item 3.48552307237904e-15\n",
       "\\item 2.78050117475054e-15\n",
       "\\item 2.29611674802203e-15\n",
       "\\item 9.86054878339099e-16\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3.81922858801525e-15\n",
       "2. 3.81922858801525e-15\n",
       "3. 3.48552307237904e-15\n",
       "4. 2.78050117475054e-15\n",
       "5. 2.29611674802203e-15\n",
       "6. 9.86054878339099e-16\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 3.819229e-15 3.819229e-15 3.485523e-15 2.780501e-15 2.296117e-15\n",
       "[6] 9.860549e-16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tail(svd(tdm)$d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "2.19766554249538e+35"
      ],
      "text/latex": [
       "2.19766554249538e+35"
      ],
      "text/markdown": [
       "2.19766554249538e+35"
      ],
      "text/plain": [
       "[1] 2.197666e+35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = as.matrix(tdm)\n",
    "xtx=t(X)%*%X\n",
    "kappa(xtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "15836739287921604"
      ],
      "text/latex": [
       "15836739287921604"
      ],
      "text/markdown": [
       "15836739287921604"
      ],
      "text/plain": [
       "[1] 1.583674e+16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max(eigen(xtx)$values)/abs(min(eigen(xtx)$values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression Solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/huntg/Documents/R/win-library/4.0'\n",
      "(as 'lib' is unspecified)\n",
      "\n",
      "also installing the dependency 'shape'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'shape' successfully unpacked and MD5 sums checked\n",
      "package 'glmnet' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\huntg\\AppData\\Local\\Temp\\Rtmpkvjs0s\\downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "install.packages('glmnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'glmnet' was built under R version 4.0.4\"\n",
      "Loading required package: Matrix\n",
      "\n",
      "Loaded glmnet 4.1-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library('glmnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for glmnet {glmnet}\"><tr><td>glmnet {glmnet}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>fit a GLM with lasso or elasticnet regularization</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>Fit a generalized linear model via penalized maximum likelihood.  The\n",
       "regularization path is computed for the lasso or elasticnet penalty at a\n",
       "grid of values for the regularization parameter lambda. Can deal with all\n",
       "shapes of data, including very large sparse data matrices. Fits linear,\n",
       "logistic and multinomial, poisson, and Cox regression models.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "glmnet(\n",
       "  x,\n",
       "  y,\n",
       "  family = c(\"gaussian\", \"binomial\", \"poisson\", \"multinomial\", \"cox\", \"mgaussian\"),\n",
       "  weights = NULL,\n",
       "  offset = NULL,\n",
       "  alpha = 1,\n",
       "  nlambda = 100,\n",
       "  lambda.min.ratio = ifelse(nobs &lt; nvars, 0.01, 1e-04),\n",
       "  lambda = NULL,\n",
       "  standardize = TRUE,\n",
       "  intercept = TRUE,\n",
       "  thresh = 1e-07,\n",
       "  dfmax = nvars + 1,\n",
       "  pmax = min(dfmax * 2 + 20, nvars),\n",
       "  exclude = NULL,\n",
       "  penalty.factor = rep(1, nvars),\n",
       "  lower.limits = -Inf,\n",
       "  upper.limits = Inf,\n",
       "  maxit = 1e+05,\n",
       "  type.gaussian = ifelse(nvars &lt; 500, \"covariance\", \"naive\"),\n",
       "  type.logistic = c(\"Newton\", \"modified.Newton\"),\n",
       "  standardize.response = FALSE,\n",
       "  type.multinomial = c(\"ungrouped\", \"grouped\"),\n",
       "  relax = FALSE,\n",
       "  trace.it = 0,\n",
       "  ...\n",
       ")\n",
       "\n",
       "relax.glmnet(fit, x, ..., maxp = n - 3, path = FALSE, check.args = TRUE)\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>x</code></td>\n",
       "<td>\n",
       "<p>input matrix, of dimension nobs x nvars; each row is an observation\n",
       "vector. Can be in sparse matrix format (inherit from class\n",
       "<code>\"sparseMatrix\"</code> as in package <code>Matrix</code>)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>y</code></td>\n",
       "<td>\n",
       "<p>response variable. Quantitative for <code>family=\"gaussian\"</code>, or\n",
       "<code>family=\"poisson\"</code> (non-negative counts). For <code>family=\"binomial\"</code>\n",
       "should be either a factor with two levels, or a two-column matrix of counts\n",
       "or proportions (the second column is treated as the target class; for a\n",
       "factor, the last level in alphabetical order is the target class). For\n",
       "<code>family=\"multinomial\"</code>, can be a <code>nc&gt;=2</code> level factor, or a matrix\n",
       "with <code>nc</code> columns of counts or proportions. For either\n",
       "<code>\"binomial\"</code> or <code>\"multinomial\"</code>, if <code>y</code> is presented as a\n",
       "vector, it will be coerced into a factor. For <code>family=\"cox\"</code>, preferably\n",
       "a <code>Surv</code> object from the survival package: see Details section for\n",
       "more information. For <code>family=\"mgaussian\"</code>, <code>y</code> is a matrix\n",
       "of quantitative responses.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>family</code></td>\n",
       "<td>\n",
       "<p>Either a character string representing\n",
       "one of the built-in families, or else a <code>glm()</code> family object. For more\n",
       "information, see Details section below or the documentation for response\n",
       "type (above).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>weights</code></td>\n",
       "<td>\n",
       "<p>observation weights. Can be total counts if responses are\n",
       "proportion matrices. Default is 1 for each observation</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>offset</code></td>\n",
       "<td>\n",
       "<p>A vector of length <code>nobs</code> that is included in the linear\n",
       "predictor (a <code>nobs x nc</code> matrix for the <code>\"multinomial\"</code> family).\n",
       "Useful for the <code>\"poisson\"</code> family (e.g. log of exposure time), or for\n",
       "refining a model by starting at a current fit. Default is <code>NULL</code>. If\n",
       "supplied, then values must also be supplied to the <code>predict</code> function.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>alpha</code></td>\n",
       "<td>\n",
       "<p>The elasticnet mixing parameter, with <i>0&le;&alpha;&le; 1</i>.\n",
       "The penalty is defined as\n",
       "</p>\n",
       "<p style=\"text-align: center;\"><i>(1-&alpha;)/2||&beta;||_2^2+&alpha;||&beta;||_1.</i></p>\n",
       " <p><code>alpha=1</code> is the\n",
       "lasso penalty, and <code>alpha=0</code> the ridge penalty.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>nlambda</code></td>\n",
       "<td>\n",
       "<p>The number of <code>lambda</code> values - default is 100.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>lambda.min.ratio</code></td>\n",
       "<td>\n",
       "<p>Smallest value for <code>lambda</code>, as a fraction of\n",
       "<code>lambda.max</code>, the (data derived) entry value (i.e. the smallest value\n",
       "for which all coefficients are zero). The default depends on the sample size\n",
       "<code>nobs</code> relative to the number of variables <code>nvars</code>. If <code>nobs\n",
       "&gt; nvars</code>, the default is <code>0.0001</code>, close to zero.  If <code>nobs &lt;\n",
       "nvars</code>, the default is <code>0.01</code>.  A very small value of\n",
       "<code>lambda.min.ratio</code> will lead to a saturated fit in the <code>nobs &lt;\n",
       "nvars</code> case. This is undefined for <code>\"binomial\"</code> and\n",
       "<code>\"multinomial\"</code> models, and <code>glmnet</code> will exit gracefully when the\n",
       "percentage deviance explained is almost 1.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>lambda</code></td>\n",
       "<td>\n",
       "<p>A user supplied <code>lambda</code> sequence. Typical usage is to\n",
       "have the program compute its own <code>lambda</code> sequence based on\n",
       "<code>nlambda</code> and <code>lambda.min.ratio</code>. Supplying a value of\n",
       "<code>lambda</code> overrides this. WARNING: use with care. Avoid supplying a\n",
       "single value for <code>lambda</code> (for predictions after CV use\n",
       "<code>predict()</code> instead).  Supply instead a decreasing sequence of\n",
       "<code>lambda</code> values. <code>glmnet</code> relies on its warms starts for speed,\n",
       "and its often faster to fit a whole path than compute a single fit.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>standardize</code></td>\n",
       "<td>\n",
       "<p>Logical flag for x variable standardization, prior to\n",
       "fitting the model sequence. The coefficients are always returned on the\n",
       "original scale. Default is <code>standardize=TRUE</code>.  If variables are in the\n",
       "same units already, you might not wish to standardize. See details below for\n",
       "y standardization with <code>family=\"gaussian\"</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>intercept</code></td>\n",
       "<td>\n",
       "<p>Should intercept(s) be fitted (default=TRUE) or set to zero\n",
       "(FALSE)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>thresh</code></td>\n",
       "<td>\n",
       "<p>Convergence threshold for coordinate descent. Each inner\n",
       "coordinate-descent loop continues until the maximum change in the objective\n",
       "after any coefficient update is less than <code>thresh</code> times the null\n",
       "deviance. Defaults value is <code>1E-7</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>dfmax</code></td>\n",
       "<td>\n",
       "<p>Limit the maximum number of variables in the model. Useful for\n",
       "very large <code>nvars</code>, if a partial path is desired.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>pmax</code></td>\n",
       "<td>\n",
       "<p>Limit the maximum number of variables ever to be nonzero</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>exclude</code></td>\n",
       "<td>\n",
       "<p>Indices of variables to be excluded from the model. Default\n",
       "is none. Equivalent to an infinite penalty factor (next item).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>penalty.factor</code></td>\n",
       "<td>\n",
       "<p>Separate penalty factors can be applied to each\n",
       "coefficient. This is a number that multiplies <code>lambda</code> to allow\n",
       "differential shrinkage. Can be 0 for some variables, which implies no\n",
       "shrinkage, and that variable is always included in the model. Default is 1\n",
       "for all variables (and implicitly infinity for variables listed in\n",
       "<code>exclude</code>). Note: the penalty factors are internally rescaled to sum to\n",
       "nvars, and the lambda sequence will reflect this change.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>lower.limits</code></td>\n",
       "<td>\n",
       "<p>Vector of lower limits for each coefficient; default\n",
       "<code>-Inf</code>. Each of these must be non-positive. Can be presented as a\n",
       "single value (which will then be replicated), else a vector of length\n",
       "<code>nvars</code></p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>upper.limits</code></td>\n",
       "<td>\n",
       "<p>Vector of upper limits for each coefficient; default\n",
       "<code>Inf</code>. See <code>lower.limits</code></p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>maxit</code></td>\n",
       "<td>\n",
       "<p>Maximum number of passes over the data for all lambda values;\n",
       "default is 10^5.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>type.gaussian</code></td>\n",
       "<td>\n",
       "<p>Two algorithm types are supported for (only)\n",
       "<code>family=\"gaussian\"</code>. The default when <code>nvar&lt;500</code> is\n",
       "<code>type.gaussian=\"covariance\"</code>, and saves all inner-products ever\n",
       "computed. This can be much faster than <code>type.gaussian=\"naive\"</code>, which\n",
       "loops through <code>nobs</code> every time an inner-product is computed. The\n",
       "latter can be far more efficient for <code>nvar &gt;&gt; nobs</code> situations, or when\n",
       "<code>nvar &gt; 500</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>type.logistic</code></td>\n",
       "<td>\n",
       "<p>If <code>\"Newton\"</code> then the exact hessian is used\n",
       "(default), while <code>\"modified.Newton\"</code> uses an upper-bound on the\n",
       "hessian, and can be faster.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>standardize.response</code></td>\n",
       "<td>\n",
       "<p>This is for the <code>family=\"mgaussian\"</code>\n",
       "family, and allows the user to standardize the response variables</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>type.multinomial</code></td>\n",
       "<td>\n",
       "<p>If <code>\"grouped\"</code> then a grouped lasso penalty is\n",
       "used on the multinomial coefficients for a variable. This ensures they are\n",
       "all in our out together. The default is <code>\"ungrouped\"</code></p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>relax</code></td>\n",
       "<td>\n",
       "<p>If <code>TRUE</code> then for each <em>active set</em> in the path of\n",
       "solutions, the model is refit without any regularization. See <code>details</code>\n",
       "for more information. This argument is new, and users may experience convergence issues\n",
       "with small datasets, especially with non-gaussian families. Limiting the\n",
       "value of 'maxp' can alleviate these issues in some cases.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>trace.it</code></td>\n",
       "<td>\n",
       "<p>If <code>trace.it=1</code>, then a progress bar is displayed;\n",
       "useful for big models that take a long time to fit.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>...</code></td>\n",
       "<td>\n",
       "<p>Additional argument used in <code>relax.glmnet</code>. These include\n",
       "some of the original arguments to 'glmnet', and each must be named if used.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>fit</code></td>\n",
       "<td>\n",
       "<p>For <code>relax.glmnet</code> a fitted 'glmnet' object</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>maxp</code></td>\n",
       "<td>\n",
       "<p>a limit on how many relaxed coefficients are allowed. Default is\n",
       "'n-3', where 'n' is the sample size. This may not be sufficient for\n",
       "non-gaussian familes, in which case users should supply a smaller value.\n",
       "This argument can be supplied directly to 'glmnet'.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>path</code></td>\n",
       "<td>\n",
       "<p>Since <code>glmnet</code> does not do stepsize optimization, the Newton\n",
       "algorithm can get stuck and not converge, especially with relaxed fits. With <code>path=TRUE</code>,\n",
       "each relaxed fit on a particular set of variables is computed pathwise using the original sequence\n",
       "of lambda values (with a zero attached to the end). Not needed for Gaussian models, and should not\n",
       "be used unless needed, since will lead to longer compute times. Default is <code>path=FALSE</code>.\n",
       "appropriate subset of variables</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>check.args</code></td>\n",
       "<td>\n",
       "<p>Should <code>relax.glmnet</code> make sure that all the data\n",
       "dependent arguments used in creating 'fit' have been resupplied. Default is\n",
       "'TRUE'.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>The sequence of models implied by <code>lambda</code> is fit by coordinate\n",
       "descent. For <code>family=\"gaussian\"</code> this is the lasso sequence if\n",
       "<code>alpha=1</code>, else it is the elasticnet sequence.\n",
       "</p>\n",
       "<p>The objective function for <code>\"gaussian\"</code> is </p>\n",
       "<p style=\"text-align: center;\"><i>1/2 RSS/nobs +\n",
       "&lambda;*penalty,</i></p>\n",
       "<p> and for the other models it is </p>\n",
       "<p style=\"text-align: center;\"><i>-loglik/nobs +\n",
       "&lambda;*penalty.</i></p>\n",
       "<p> Note also that for <code>\"gaussian\"</code>, <code>glmnet</code>\n",
       "standardizes y to have unit variance (using 1/n rather than 1/(n-1) formula)\n",
       "before computing its lambda sequence (and then unstandardizes the resulting\n",
       "coefficients); if you wish to reproduce/compare results with other software,\n",
       "best to supply a standardized y. The coefficients for any predictor\n",
       "variables with zero variance are set to zero for all values of lambda.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h4>Details on <code>family</code> option</h4>\n",
       "\n",
       "<p>From version 4.0 onwards, glmnet supports both the original built-in families,\n",
       "as well as <em>any</em> family object as used by <code>stats:glm()</code>.\n",
       "This opens the door to a wide variety of additional models. For example\n",
       "<code>family=binomial(link=cloglog)</code> or <code>family=negative.binomial(theta=1.5)</code> (from the MASS library).\n",
       "Note that the code runs faster for the built-in families.\n",
       "</p>\n",
       "<p>The built in families are specifed via a character string. For all families,\n",
       "the object produced is a lasso or elasticnet regularization path for fitting the\n",
       "generalized linear regression paths, by maximizing the appropriate penalized\n",
       "log-likelihood (partial likelihood for the &quot;cox&quot; model). Sometimes the\n",
       "sequence is truncated before <code>nlambda</code> values of <code>lambda</code> have\n",
       "been used, because of instabilities in the inverse link functions near a\n",
       "saturated fit. <code>glmnet(...,family=\"binomial\")</code> fits a traditional\n",
       "logistic regression model for the log-odds.\n",
       "<code>glmnet(...,family=\"multinomial\")</code> fits a symmetric multinomial model,\n",
       "where each class is represented by a linear model (on the log-scale). The\n",
       "penalties take care of redundancies. A two-class <code>\"multinomial\"</code> model\n",
       "will produce the same fit as the corresponding <code>\"binomial\"</code> model,\n",
       "except the pair of coefficient matrices will be equal in magnitude and\n",
       "opposite in sign, and half the <code>\"binomial\"</code> values.\n",
       "Two useful additional families are the <code>family=\"mgaussian\"</code> family and\n",
       "the <code>type.multinomial=\"grouped\"</code> option for multinomial fitting. The\n",
       "former allows a multi-response gaussian model to be fit, using a &quot;group\n",
       "-lasso&quot; penalty on the coefficients for each variable. Tying the responses\n",
       "together like this is called &quot;multi-task&quot; learning in some domains. The\n",
       "grouped multinomial allows the same penalty for the\n",
       "<code>family=\"multinomial\"</code> model, which is also multi-responsed. For both\n",
       "of these the penalty on the coefficient vector for variable j is\n",
       "</p>\n",
       "<p style=\"text-align: center;\"><i>(1-&alpha;)/2||&beta;_j||_2^2+&alpha;||&beta;_j||_2.</i></p>\n",
       "<p> When <code>alpha=1</code>\n",
       "this is a group-lasso penalty, and otherwise it mixes with quadratic just\n",
       "like elasticnet. A small detail in the Cox model: if death times are tied\n",
       "with censored times, we assume the censored times occurred just\n",
       "<em>before</em> the death times in computing the Breslow approximation; if\n",
       "users prefer the usual convention of <em>after</em>, they can add a small\n",
       "number to all censoring times to achieve this effect.\n",
       "</p>\n",
       "\n",
       "\n",
       "\n",
       "<h4>Details on response for <code>family=\"cox\"</code></h4>\n",
       "\n",
       "<p>For Cox models, the response should preferably be a <code>Surv</code> object,\n",
       "created by the <code>Surv()</code> function in <span class=\"pkg\">survival</span> package. For\n",
       "right-censored data, this object should have type &quot;right&quot;, and for\n",
       "(start, stop] data, it should have type &quot;counting&quot;. To fit stratified Cox\n",
       "models, strata should be added to the response via the <code>stratifySurv()</code>\n",
       "function before passing the response to <code>glmnet()</code>. (For backward\n",
       "compatibility, right-censored data can also be passed as a\n",
       "two-column matrix with columns named 'time' and 'status'. The\n",
       "latter is a binary variable, with '1' indicating death, and '0' indicating\n",
       "right censored.)\n",
       "</p>\n",
       "\n",
       "\n",
       "\n",
       "<h4>Details on <code>relax</code> option</h4>\n",
       "\n",
       "<p>If <code>relax=TRUE</code>\n",
       "a duplicate sequence of models is produced, where each active set in the\n",
       "elastic-net path is refit without regularization. The result of this is a\n",
       "matching <code>\"glmnet\"</code> object which is stored on the original object in a\n",
       "component named <code>\"relaxed\"</code>, and is part of the glmnet output.\n",
       "Generally users will not call <code>relax.glmnet</code> directly, unless the\n",
       "original 'glmnet' object took a long time to fit. But if they do, they must\n",
       "supply the fit, and all the original arguments used to create that fit. They\n",
       "can limit the length of the relaxed path via 'maxp'.\n",
       "</p>\n",
       "\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>An object with S3 class <code>\"glmnet\",\"*\" </code>, where <code>\"*\"</code> is\n",
       "<code>\"elnet\"</code>, <code>\"lognet\"</code>, <code>\"multnet\"</code>, <code>\"fishnet\"</code>\n",
       "(poisson), <code>\"coxnet\"</code> or <code>\"mrelnet\"</code> for the various types of\n",
       "models. If the model was created with <code>relax=TRUE</code> then this class has\n",
       "a prefix class of <code>\"relaxed\"</code>.  </p>\n",
       "<table summary=\"R valueblock\">\n",
       "<tr valign=\"top\"><td><code>call</code></td>\n",
       "<td>\n",
       "<p>the call that produced this\n",
       "object</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>a0</code></td>\n",
       "<td>\n",
       "<p>Intercept sequence of length <code>length(lambda)</code></p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>beta</code></td>\n",
       "<td>\n",
       "<p>For <code>\"elnet\"</code>, <code>\"lognet\"</code>, <code>\"fishnet\"</code> and\n",
       "<code>\"coxnet\"</code> models, a <code>nvars x length(lambda)</code> matrix of\n",
       "coefficients, stored in sparse column format (<code>\"CsparseMatrix\"</code>). For\n",
       "<code>\"multnet\"</code> and <code>\"mgaussian\"</code>, a list of <code>nc</code> such matrices,\n",
       "one for each class.</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>lambda</code></td>\n",
       "<td>\n",
       "<p>The actual sequence of <code>lambda</code>\n",
       "values used. When <code>alpha=0</code>, the largest lambda reported does not quite\n",
       "give the zero coefficients reported (<code>lambda=inf</code> would in principle).\n",
       "Instead, the largest <code>lambda</code> for <code>alpha=0.001</code> is used, and the\n",
       "sequence of <code>lambda</code> values is derived from this.</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>dev.ratio</code></td>\n",
       "<td>\n",
       "<p>The\n",
       "fraction of (null) deviance explained (for <code>\"elnet\"</code>, this is the\n",
       "R-square). The deviance calculations incorporate weights if present in the\n",
       "model. The deviance is defined to be 2*(loglike_sat - loglike), where\n",
       "loglike_sat is the log-likelihood for the saturated model (a model with a\n",
       "free parameter per observation). Hence dev.ratio=1-dev/nulldev.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>nulldev</code></td>\n",
       "<td>\n",
       "<p>Null deviance (per observation). This is defined to be\n",
       "2*(loglike_sat -loglike(Null)); The NULL model refers to the intercept\n",
       "model, except for the Cox, where it is the 0 model.</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>df</code></td>\n",
       "<td>\n",
       "<p>The number of\n",
       "nonzero coefficients for each value of <code>lambda</code>. For <code>\"multnet\"</code>,\n",
       "this is the number of variables with a nonzero coefficient for <em>any</em>\n",
       "class.</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>dfmat</code></td>\n",
       "<td>\n",
       "<p>For <code>\"multnet\"</code> and <code>\"mrelnet\"</code> only. A\n",
       "matrix consisting of the number of nonzero coefficients per class</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>dim</code></td>\n",
       "<td>\n",
       "<p>dimension of coefficient matrix (ices)</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>nobs</code></td>\n",
       "<td>\n",
       "<p>number of\n",
       "observations</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>npasses</code></td>\n",
       "<td>\n",
       "<p>total passes over the data summed over all\n",
       "lambda values</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>offset</code></td>\n",
       "<td>\n",
       "<p>a logical variable indicating whether an offset\n",
       "was included in the model</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>jerr</code></td>\n",
       "<td>\n",
       "<p>error flag, for warnings and errors\n",
       "(largely for internal debugging).</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>relaxed</code></td>\n",
       "<td>\n",
       "<p>If <code>relax=TRUE</code>, this\n",
       "additional item is another glmnet object with different values for\n",
       "<code>beta</code> and <code>dev.ratio</code></p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Author(s)</h3>\n",
       "\n",
       "<p>Jerome Friedman, Trevor Hastie, Balasubramanian Narasimhan, Noah\n",
       "Simon, Kenneth Tay and Rob Tibshirani<br /> Maintainer: Trevor Hastie\n",
       "<a href=\"mailto:hastie@stanford.edu\">hastie@stanford.edu</a>\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Friedman, J., Hastie, T. and Tibshirani, R. (2008)\n",
       "<em>Regularization Paths for Generalized Linear Models via Coordinate\n",
       "Descent (2010), Journal of Statistical Software, Vol. 33(1), 1-22</em>,\n",
       "<a href=\"https://web.stanford.edu/~hastie/Papers/glmnet.pdf\">https://web.stanford.edu/~hastie/Papers/glmnet.pdf</a>.<br />\n",
       "Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2011)\n",
       "<em>Regularization Paths for Cox's Proportional\n",
       "Hazards Model via Coordinate Descent, Journal of Statistical Software, Vol.\n",
       "39(5), 1-13</em>, <a href=\"https://www.jstatsoft.org/v39/i05/\">https://www.jstatsoft.org/v39/i05/</a>.<br /> Tibshirani,\n",
       "Robert, Bien, J., Friedman, J., Hastie, T.,Simon, N.,Taylor, J. and\n",
       "Tibshirani, Ryan. (2012) <em>Strong Rules for Discarding Predictors in\n",
       "Lasso-type Problems, JRSSB, Vol. 74(2), 245-266</em>,\n",
       "<a href=\"https://statweb.stanford.edu/~tibs/ftp/strong.pdf\">https://statweb.stanford.edu/~tibs/ftp/strong.pdf</a>.<br />\n",
       "Hastie, T., Tibshirani, Robert and Tibshirani, Ryan. <em>Extended\n",
       "Comparisons of Best Subset Selection, Forward Stepwise Selection, and the\n",
       "Lasso (2017), Stanford Statistics Technical Report</em>,\n",
       "<a href=\"https://arxiv.org/abs/1707.08692\">https://arxiv.org/abs/1707.08692</a>.<br />\n",
       "Glmnet webpage with four vignettes, <a href=\"https://glmnet.stanford.edu\">https://glmnet.stanford.edu</a>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>print</code>, <code>predict</code>, <code>coef</code> and <code>plot</code> methods,\n",
       "and the <code>cv.glmnet</code> function.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "\n",
       "# Gaussian\n",
       "x = matrix(rnorm(100 * 20), 100, 20)\n",
       "y = rnorm(100)\n",
       "fit1 = glmnet(x, y)\n",
       "print(fit1)\n",
       "coef(fit1, s = 0.01)  # extract coefficients at a single value of lambda\n",
       "predict(fit1, newx = x[1:10, ], s = c(0.01, 0.005))  # make predictions\n",
       "\n",
       "# Relaxed\n",
       "fit1r = glmnet(x, y, relax = TRUE)  # can be used with any model\n",
       "\n",
       "# multivariate gaussian\n",
       "y = matrix(rnorm(100 * 3), 100, 3)\n",
       "fit1m = glmnet(x, y, family = \"mgaussian\")\n",
       "plot(fit1m, type.coef = \"2norm\")\n",
       "\n",
       "# binomial\n",
       "g2 = sample(c(0,1), 100, replace = TRUE)\n",
       "fit2 = glmnet(x, g2, family = \"binomial\")\n",
       "fit2n = glmnet(x, g2, family = binomial(link=cloglog))\n",
       "fit2r = glmnet(x,g2, family = \"binomial\", relax=TRUE)\n",
       "fit2rp = glmnet(x,g2, family = \"binomial\", relax=TRUE, path=TRUE)\n",
       "\n",
       "# multinomial\n",
       "g4 = sample(1:4, 100, replace = TRUE)\n",
       "fit3 = glmnet(x, g4, family = \"multinomial\")\n",
       "fit3a = glmnet(x, g4, family = \"multinomial\", type.multinomial = \"grouped\")\n",
       "# poisson\n",
       "N = 500\n",
       "p = 20\n",
       "nzc = 5\n",
       "x = matrix(rnorm(N * p), N, p)\n",
       "beta = rnorm(nzc)\n",
       "f = x[, seq(nzc)] %*% beta\n",
       "mu = exp(f)\n",
       "y = rpois(N, mu)\n",
       "fit = glmnet(x, y, family = \"poisson\")\n",
       "plot(fit)\n",
       "pfit = predict(fit, x, s = 0.001, type = \"response\")\n",
       "plot(pfit, y)\n",
       "\n",
       "# Cox\n",
       "set.seed(10101)\n",
       "N = 1000\n",
       "p = 30\n",
       "nzc = p/3\n",
       "x = matrix(rnorm(N * p), N, p)\n",
       "beta = rnorm(nzc)\n",
       "fx = x[, seq(nzc)] %*% beta/3\n",
       "hx = exp(fx)\n",
       "ty = rexp(N, hx)\n",
       "tcens = rbinom(n = N, prob = 0.3, size = 1)  # censoring indicator\n",
       "y = cbind(time = ty, status = 1 - tcens)  # y=Surv(ty,1-tcens) with library(survival)\n",
       "fit = glmnet(x, y, family = \"cox\")\n",
       "plot(fit)\n",
       "\n",
       "# Cox example with (start, stop] data\n",
       "set.seed(2)\n",
       "nobs &lt;- 100; nvars &lt;- 15\n",
       "xvec &lt;- rnorm(nobs * nvars)\n",
       "xvec[sample.int(nobs * nvars, size = 0.4 * nobs * nvars)] &lt;- 0\n",
       "x &lt;- matrix(xvec, nrow = nobs)\n",
       "start_time &lt;- runif(100, min = 0, max = 5)\n",
       "stop_time &lt;- start_time + runif(100, min = 0.1, max = 3)\n",
       "status &lt;- rbinom(n = nobs, prob = 0.3, size = 1)\n",
       "jsurv_ss &lt;- survival::Surv(start_time, stop_time, status)\n",
       "fit &lt;- glmnet(x, jsurv_ss, family = \"cox\")\n",
       "\n",
       "# Cox example with strata\n",
       "jsurv_ss2 &lt;- stratifySurv(jsurv_ss, rep(1:2, each = 50))\n",
       "fit &lt;- glmnet(x, jsurv_ss2, family = \"cox\")\n",
       "\n",
       "# Sparse\n",
       "n = 10000\n",
       "p = 200\n",
       "nzc = trunc(p/10)\n",
       "x = matrix(rnorm(n * p), n, p)\n",
       "iz = sample(1:(n * p), size = n * p * 0.85, replace = FALSE)\n",
       "x[iz] = 0\n",
       "sx = Matrix(x, sparse = TRUE)\n",
       "inherits(sx, \"sparseMatrix\")  #confirm that it is sparse\n",
       "beta = rnorm(nzc)\n",
       "fx = x[, seq(nzc)] %*% beta\n",
       "eps = rnorm(n)\n",
       "y = fx + eps\n",
       "px = exp(fx)\n",
       "px = px/(1 + px)\n",
       "ly = rbinom(n = length(px), prob = px, size = 1)\n",
       "system.time(fit1 &lt;- glmnet(sx, y))\n",
       "system.time(fit2n &lt;- glmnet(x, y))\n",
       "\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>glmnet</em> version 4.1-1 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{glmnet}{fit a GLM with lasso or elasticnet regularization}{glmnet}\n",
       "\\aliasA{relax.glmnet}{glmnet}{relax.glmnet}\n",
       "\\keyword{models}{glmnet}\n",
       "\\keyword{regression}{glmnet}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "Fit a generalized linear model via penalized maximum likelihood.  The\n",
       "regularization path is computed for the lasso or elasticnet penalty at a\n",
       "grid of values for the regularization parameter lambda. Can deal with all\n",
       "shapes of data, including very large sparse data matrices. Fits linear,\n",
       "logistic and multinomial, poisson, and Cox regression models.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "glmnet(\n",
       "  x,\n",
       "  y,\n",
       "  family = c(\"gaussian\", \"binomial\", \"poisson\", \"multinomial\", \"cox\", \"mgaussian\"),\n",
       "  weights = NULL,\n",
       "  offset = NULL,\n",
       "  alpha = 1,\n",
       "  nlambda = 100,\n",
       "  lambda.min.ratio = ifelse(nobs < nvars, 0.01, 1e-04),\n",
       "  lambda = NULL,\n",
       "  standardize = TRUE,\n",
       "  intercept = TRUE,\n",
       "  thresh = 1e-07,\n",
       "  dfmax = nvars + 1,\n",
       "  pmax = min(dfmax * 2 + 20, nvars),\n",
       "  exclude = NULL,\n",
       "  penalty.factor = rep(1, nvars),\n",
       "  lower.limits = -Inf,\n",
       "  upper.limits = Inf,\n",
       "  maxit = 1e+05,\n",
       "  type.gaussian = ifelse(nvars < 500, \"covariance\", \"naive\"),\n",
       "  type.logistic = c(\"Newton\", \"modified.Newton\"),\n",
       "  standardize.response = FALSE,\n",
       "  type.multinomial = c(\"ungrouped\", \"grouped\"),\n",
       "  relax = FALSE,\n",
       "  trace.it = 0,\n",
       "  ...\n",
       ")\n",
       "\n",
       "relax.glmnet(fit, x, ..., maxp = n - 3, path = FALSE, check.args = TRUE)\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{x}] input matrix, of dimension nobs x nvars; each row is an observation\n",
       "vector. Can be in sparse matrix format (inherit from class\n",
       "\\code{\"sparseMatrix\"} as in package \\code{Matrix})\n",
       "\n",
       "\\item[\\code{y}] response variable. Quantitative for \\code{family=\"gaussian\"}, or\n",
       "\\code{family=\"poisson\"} (non-negative counts). For \\code{family=\"binomial\"}\n",
       "should be either a factor with two levels, or a two-column matrix of counts\n",
       "or proportions (the second column is treated as the target class; for a\n",
       "factor, the last level in alphabetical order is the target class). For\n",
       "\\code{family=\"multinomial\"}, can be a \\code{nc>=2} level factor, or a matrix\n",
       "with \\code{nc} columns of counts or proportions. For either\n",
       "\\code{\"binomial\"} or \\code{\"multinomial\"}, if \\code{y} is presented as a\n",
       "vector, it will be coerced into a factor. For \\code{family=\"cox\"}, preferably\n",
       "a \\code{Surv} object from the survival package: see Details section for\n",
       "more information. For \\code{family=\"mgaussian\"}, \\code{y} is a matrix\n",
       "of quantitative responses.\n",
       "\n",
       "\\item[\\code{family}] Either a character string representing\n",
       "one of the built-in families, or else a \\code{glm()} family object. For more\n",
       "information, see Details section below or the documentation for response\n",
       "type (above).\n",
       "\n",
       "\\item[\\code{weights}] observation weights. Can be total counts if responses are\n",
       "proportion matrices. Default is 1 for each observation\n",
       "\n",
       "\\item[\\code{offset}] A vector of length \\code{nobs} that is included in the linear\n",
       "predictor (a \\code{nobs x nc} matrix for the \\code{\"multinomial\"} family).\n",
       "Useful for the \\code{\"poisson\"} family (e.g. log of exposure time), or for\n",
       "refining a model by starting at a current fit. Default is \\code{NULL}. If\n",
       "supplied, then values must also be supplied to the \\code{predict} function.\n",
       "\n",
       "\\item[\\code{alpha}] The elasticnet mixing parameter, with \\eqn{0\\le\\alpha\\le 1}{}.\n",
       "The penalty is defined as\n",
       "\\deqn{(1-\\alpha)/2||\\beta||_2^2+\\alpha||\\beta||_1.}{} \\code{alpha=1} is the\n",
       "lasso penalty, and \\code{alpha=0} the ridge penalty.\n",
       "\n",
       "\\item[\\code{nlambda}] The number of \\code{lambda} values - default is 100.\n",
       "\n",
       "\\item[\\code{lambda.min.ratio}] Smallest value for \\code{lambda}, as a fraction of\n",
       "\\code{lambda.max}, the (data derived) entry value (i.e. the smallest value\n",
       "for which all coefficients are zero). The default depends on the sample size\n",
       "\\code{nobs} relative to the number of variables \\code{nvars}. If \\code{nobs\n",
       "> nvars}, the default is \\code{0.0001}, close to zero.  If \\code{nobs <\n",
       "nvars}, the default is \\code{0.01}.  A very small value of\n",
       "\\code{lambda.min.ratio} will lead to a saturated fit in the \\code{nobs <\n",
       "nvars} case. This is undefined for \\code{\"binomial\"} and\n",
       "\\code{\"multinomial\"} models, and \\code{glmnet} will exit gracefully when the\n",
       "percentage deviance explained is almost 1.\n",
       "\n",
       "\\item[\\code{lambda}] A user supplied \\code{lambda} sequence. Typical usage is to\n",
       "have the program compute its own \\code{lambda} sequence based on\n",
       "\\code{nlambda} and \\code{lambda.min.ratio}. Supplying a value of\n",
       "\\code{lambda} overrides this. WARNING: use with care. Avoid supplying a\n",
       "single value for \\code{lambda} (for predictions after CV use\n",
       "\\code{predict()} instead).  Supply instead a decreasing sequence of\n",
       "\\code{lambda} values. \\code{glmnet} relies on its warms starts for speed,\n",
       "and its often faster to fit a whole path than compute a single fit.\n",
       "\n",
       "\\item[\\code{standardize}] Logical flag for x variable standardization, prior to\n",
       "fitting the model sequence. The coefficients are always returned on the\n",
       "original scale. Default is \\code{standardize=TRUE}.  If variables are in the\n",
       "same units already, you might not wish to standardize. See details below for\n",
       "y standardization with \\code{family=\"gaussian\"}.\n",
       "\n",
       "\\item[\\code{intercept}] Should intercept(s) be fitted (default=TRUE) or set to zero\n",
       "(FALSE)\n",
       "\n",
       "\\item[\\code{thresh}] Convergence threshold for coordinate descent. Each inner\n",
       "coordinate-descent loop continues until the maximum change in the objective\n",
       "after any coefficient update is less than \\code{thresh} times the null\n",
       "deviance. Defaults value is \\code{1E-7}.\n",
       "\n",
       "\\item[\\code{dfmax}] Limit the maximum number of variables in the model. Useful for\n",
       "very large \\code{nvars}, if a partial path is desired.\n",
       "\n",
       "\\item[\\code{pmax}] Limit the maximum number of variables ever to be nonzero\n",
       "\n",
       "\\item[\\code{exclude}] Indices of variables to be excluded from the model. Default\n",
       "is none. Equivalent to an infinite penalty factor (next item).\n",
       "\n",
       "\\item[\\code{penalty.factor}] Separate penalty factors can be applied to each\n",
       "coefficient. This is a number that multiplies \\code{lambda} to allow\n",
       "differential shrinkage. Can be 0 for some variables, which implies no\n",
       "shrinkage, and that variable is always included in the model. Default is 1\n",
       "for all variables (and implicitly infinity for variables listed in\n",
       "\\code{exclude}). Note: the penalty factors are internally rescaled to sum to\n",
       "nvars, and the lambda sequence will reflect this change.\n",
       "\n",
       "\\item[\\code{lower.limits}] Vector of lower limits for each coefficient; default\n",
       "\\code{-Inf}. Each of these must be non-positive. Can be presented as a\n",
       "single value (which will then be replicated), else a vector of length\n",
       "\\code{nvars}\n",
       "\n",
       "\\item[\\code{upper.limits}] Vector of upper limits for each coefficient; default\n",
       "\\code{Inf}. See \\code{lower.limits}\n",
       "\n",
       "\\item[\\code{maxit}] Maximum number of passes over the data for all lambda values;\n",
       "default is 10\\textasciicircum{}5.\n",
       "\n",
       "\\item[\\code{type.gaussian}] Two algorithm types are supported for (only)\n",
       "\\code{family=\"gaussian\"}. The default when \\code{nvar<500} is\n",
       "\\code{type.gaussian=\"covariance\"}, and saves all inner-products ever\n",
       "computed. This can be much faster than \\code{type.gaussian=\"naive\"}, which\n",
       "loops through \\code{nobs} every time an inner-product is computed. The\n",
       "latter can be far more efficient for \\code{nvar >{}> nobs} situations, or when\n",
       "\\code{nvar > 500}.\n",
       "\n",
       "\\item[\\code{type.logistic}] If \\code{\"Newton\"} then the exact hessian is used\n",
       "(default), while \\code{\"modified.Newton\"} uses an upper-bound on the\n",
       "hessian, and can be faster.\n",
       "\n",
       "\\item[\\code{standardize.response}] This is for the \\code{family=\"mgaussian\"}\n",
       "family, and allows the user to standardize the response variables\n",
       "\n",
       "\\item[\\code{type.multinomial}] If \\code{\"grouped\"} then a grouped lasso penalty is\n",
       "used on the multinomial coefficients for a variable. This ensures they are\n",
       "all in our out together. The default is \\code{\"ungrouped\"}\n",
       "\n",
       "\\item[\\code{relax}] If \\code{TRUE} then for each \\emph{active set} in the path of\n",
       "solutions, the model is refit without any regularization. See \\code{details}\n",
       "for more information. This argument is new, and users may experience convergence issues\n",
       "with small datasets, especially with non-gaussian families. Limiting the\n",
       "value of 'maxp' can alleviate these issues in some cases.\n",
       "\n",
       "\\item[\\code{trace.it}] If \\code{trace.it=1}, then a progress bar is displayed;\n",
       "useful for big models that take a long time to fit.\n",
       "\n",
       "\\item[\\code{...}] Additional argument used in \\code{relax.glmnet}. These include\n",
       "some of the original arguments to 'glmnet', and each must be named if used.\n",
       "\n",
       "\\item[\\code{fit}] For \\code{relax.glmnet} a fitted 'glmnet' object\n",
       "\n",
       "\\item[\\code{maxp}] a limit on how many relaxed coefficients are allowed. Default is\n",
       "'n-3', where 'n' is the sample size. This may not be sufficient for\n",
       "non-gaussian familes, in which case users should supply a smaller value.\n",
       "This argument can be supplied directly to 'glmnet'.\n",
       "\n",
       "\\item[\\code{path}] Since \\code{glmnet} does not do stepsize optimization, the Newton\n",
       "algorithm can get stuck and not converge, especially with relaxed fits. With \\code{path=TRUE},\n",
       "each relaxed fit on a particular set of variables is computed pathwise using the original sequence\n",
       "of lambda values (with a zero attached to the end). Not needed for Gaussian models, and should not\n",
       "be used unless needed, since will lead to longer compute times. Default is \\code{path=FALSE}.\n",
       "appropriate subset of variables\n",
       "\n",
       "\\item[\\code{check.args}] Should \\code{relax.glmnet} make sure that all the data\n",
       "dependent arguments used in creating 'fit' have been resupplied. Default is\n",
       "'TRUE'.\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "The sequence of models implied by \\code{lambda} is fit by coordinate\n",
       "descent. For \\code{family=\"gaussian\"} this is the lasso sequence if\n",
       "\\code{alpha=1}, else it is the elasticnet sequence.\n",
       "\n",
       "The objective function for \\code{\"gaussian\"} is \\deqn{1/2 RSS/nobs +\n",
       "\\lambda*penalty,}{} and for the other models it is \\deqn{-loglik/nobs +\n",
       "\\lambda*penalty.}{} Note also that for \\code{\"gaussian\"}, \\code{glmnet}\n",
       "standardizes y to have unit variance (using 1/n rather than 1/(n-1) formula)\n",
       "before computing its lambda sequence (and then unstandardizes the resulting\n",
       "coefficients); if you wish to reproduce/compare results with other software,\n",
       "best to supply a standardized y. The coefficients for any predictor\n",
       "variables with zero variance are set to zero for all values of lambda.\n",
       "%\n",
       "\\begin{SubSection}{Details on \\code{family} option}\n",
       "\n",
       "From version 4.0 onwards, glmnet supports both the original built-in families,\n",
       "as well as \\emph{any} family object as used by \\code{stats:glm()}.\n",
       "This opens the door to a wide variety of additional models. For example\n",
       "\\code{family=binomial(link=cloglog)} or \\code{family=negative.binomial(theta=1.5)} (from the MASS library).\n",
       "Note that the code runs faster for the built-in families.\n",
       "\n",
       "The built in families are specifed via a character string. For all families,\n",
       "the object produced is a lasso or elasticnet regularization path for fitting the\n",
       "generalized linear regression paths, by maximizing the appropriate penalized\n",
       "log-likelihood (partial likelihood for the \"cox\" model). Sometimes the\n",
       "sequence is truncated before \\code{nlambda} values of \\code{lambda} have\n",
       "been used, because of instabilities in the inverse link functions near a\n",
       "saturated fit. \\code{glmnet(...,family=\"binomial\")} fits a traditional\n",
       "logistic regression model for the log-odds.\n",
       "\\code{glmnet(...,family=\"multinomial\")} fits a symmetric multinomial model,\n",
       "where each class is represented by a linear model (on the log-scale). The\n",
       "penalties take care of redundancies. A two-class \\code{\"multinomial\"} model\n",
       "will produce the same fit as the corresponding \\code{\"binomial\"} model,\n",
       "except the pair of coefficient matrices will be equal in magnitude and\n",
       "opposite in sign, and half the \\code{\"binomial\"} values.\n",
       "Two useful additional families are the \\code{family=\"mgaussian\"} family and\n",
       "the \\code{type.multinomial=\"grouped\"} option for multinomial fitting. The\n",
       "former allows a multi-response gaussian model to be fit, using a \"group\n",
       "-lasso\" penalty on the coefficients for each variable. Tying the responses\n",
       "together like this is called \"multi-task\" learning in some domains. The\n",
       "grouped multinomial allows the same penalty for the\n",
       "\\code{family=\"multinomial\"} model, which is also multi-responsed. For both\n",
       "of these the penalty on the coefficient vector for variable j is\n",
       "\\deqn{(1-\\alpha)/2||\\beta_j||_2^2+\\alpha||\\beta_j||_2.}{} When \\code{alpha=1}\n",
       "this is a group-lasso penalty, and otherwise it mixes with quadratic just\n",
       "like elasticnet. A small detail in the Cox model: if death times are tied\n",
       "with censored times, we assume the censored times occurred just\n",
       "\\emph{before} the death times in computing the Breslow approximation; if\n",
       "users prefer the usual convention of \\emph{after}, they can add a small\n",
       "number to all censoring times to achieve this effect.\n",
       "\\end{SubSection}\n",
       "\n",
       "\n",
       "%\n",
       "\\begin{SubSection}{Details on response for \\code{family=\"cox\"}}\n",
       "\n",
       "For Cox models, the response should preferably be a \\code{Surv} object,\n",
       "created by the \\code{Surv()} function in \\pkg{survival} package. For\n",
       "right-censored data, this object should have type \"right\", and for\n",
       "(start, stop] data, it should have type \"counting\". To fit stratified Cox\n",
       "models, strata should be added to the response via the \\code{stratifySurv()}\n",
       "function before passing the response to \\code{glmnet()}. (For backward\n",
       "compatibility, right-censored data can also be passed as a\n",
       "two-column matrix with columns named 'time' and 'status'. The\n",
       "latter is a binary variable, with '1' indicating death, and '0' indicating\n",
       "right censored.)\n",
       "\\end{SubSection}\n",
       "\n",
       "\n",
       "%\n",
       "\\begin{SubSection}{Details on \\code{relax} option}\n",
       "\n",
       "If \\code{relax=TRUE}\n",
       "a duplicate sequence of models is produced, where each active set in the\n",
       "elastic-net path is refit without regularization. The result of this is a\n",
       "matching \\code{\"glmnet\"} object which is stored on the original object in a\n",
       "component named \\code{\"relaxed\"}, and is part of the glmnet output.\n",
       "Generally users will not call \\code{relax.glmnet} directly, unless the\n",
       "original 'glmnet' object took a long time to fit. But if they do, they must\n",
       "supply the fit, and all the original arguments used to create that fit. They\n",
       "can limit the length of the relaxed path via 'maxp'.\n",
       "\\end{SubSection}\n",
       "\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "An object with S3 class \\code{\"glmnet\",\"*\" }, where \\code{\"*\"} is\n",
       "\\code{\"elnet\"}, \\code{\"lognet\"}, \\code{\"multnet\"}, \\code{\"fishnet\"}\n",
       "(poisson), \\code{\"coxnet\"} or \\code{\"mrelnet\"} for the various types of\n",
       "models. If the model was created with \\code{relax=TRUE} then this class has\n",
       "a prefix class of \\code{\"relaxed\"}.  \\begin{ldescription}\n",
       "\\item[\\code{call}] the call that produced this\n",
       "object\\item[\\code{a0}] Intercept sequence of length \\code{length(lambda)}\n",
       "\\item[\\code{beta}] For \\code{\"elnet\"}, \\code{\"lognet\"}, \\code{\"fishnet\"} and\n",
       "\\code{\"coxnet\"} models, a \\code{nvars x length(lambda)} matrix of\n",
       "coefficients, stored in sparse column format (\\code{\"CsparseMatrix\"}). For\n",
       "\\code{\"multnet\"} and \\code{\"mgaussian\"}, a list of \\code{nc} such matrices,\n",
       "one for each class.\\item[\\code{lambda}] The actual sequence of \\code{lambda}\n",
       "values used. When \\code{alpha=0}, the largest lambda reported does not quite\n",
       "give the zero coefficients reported (\\code{lambda=inf} would in principle).\n",
       "Instead, the largest \\code{lambda} for \\code{alpha=0.001} is used, and the\n",
       "sequence of \\code{lambda} values is derived from this.\\item[\\code{dev.ratio}] The\n",
       "fraction of (null) deviance explained (for \\code{\"elnet\"}, this is the\n",
       "R-square). The deviance calculations incorporate weights if present in the\n",
       "model. The deviance is defined to be 2*(loglike\\_sat - loglike), where\n",
       "loglike\\_sat is the log-likelihood for the saturated model (a model with a\n",
       "free parameter per observation). Hence dev.ratio=1-dev/nulldev.\n",
       "\\item[\\code{nulldev}] Null deviance (per observation). This is defined to be\n",
       "2*(loglike\\_sat -loglike(Null)); The NULL model refers to the intercept\n",
       "model, except for the Cox, where it is the 0 model.\\item[\\code{df}] The number of\n",
       "nonzero coefficients for each value of \\code{lambda}. For \\code{\"multnet\"},\n",
       "this is the number of variables with a nonzero coefficient for \\emph{any}\n",
       "class.\\item[\\code{dfmat}] For \\code{\"multnet\"} and \\code{\"mrelnet\"} only. A\n",
       "matrix consisting of the number of nonzero coefficients per class\n",
       "\\item[\\code{dim}] dimension of coefficient matrix (ices)\\item[\\code{nobs}] number of\n",
       "observations\\item[\\code{npasses}] total passes over the data summed over all\n",
       "lambda values\\item[\\code{offset}] a logical variable indicating whether an offset\n",
       "was included in the model\\item[\\code{jerr}] error flag, for warnings and errors\n",
       "(largely for internal debugging).\\item[\\code{relaxed}] If \\code{relax=TRUE}, this\n",
       "additional item is another glmnet object with different values for\n",
       "\\code{beta} and \\code{dev.ratio}\n",
       "\\end{ldescription}\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Author}\\relax\n",
       "Jerome Friedman, Trevor Hastie, Balasubramanian Narasimhan, Noah\n",
       "Simon, Kenneth Tay and Rob Tibshirani\\\\{} Maintainer: Trevor Hastie\n",
       "\\email{hastie@stanford.edu}\n",
       "\\end{Author}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Friedman, J., Hastie, T. and Tibshirani, R. (2008)\n",
       "\\emph{Regularization Paths for Generalized Linear Models via Coordinate\n",
       "Descent (2010), Journal of Statistical Software, Vol. 33(1), 1-22},\n",
       "\\url{https://web.stanford.edu/~hastie/Papers/glmnet.pdf}.\\\\{}\n",
       "Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2011)\n",
       "\\emph{Regularization Paths for Cox's Proportional\n",
       "Hazards Model via Coordinate Descent, Journal of Statistical Software, Vol.\n",
       "39(5), 1-13}, \\url{https://www.jstatsoft.org/v39/i05/}.\\\\{} Tibshirani,\n",
       "Robert, Bien, J., Friedman, J., Hastie, T.,Simon, N.,Taylor, J. and\n",
       "Tibshirani, Ryan. (2012) \\emph{Strong Rules for Discarding Predictors in\n",
       "Lasso-type Problems, JRSSB, Vol. 74(2), 245-266},\n",
       "\\url{https://statweb.stanford.edu/~tibs/ftp/strong.pdf}.\\\\{}\n",
       "Hastie, T., Tibshirani, Robert and Tibshirani, Ryan. \\emph{Extended\n",
       "Comparisons of Best Subset Selection, Forward Stepwise Selection, and the\n",
       "Lasso (2017), Stanford Statistics Technical Report},\n",
       "\\url{https://arxiv.org/abs/1707.08692}.\\\\{}\n",
       "Glmnet webpage with four vignettes, \\url{https://glmnet.stanford.edu}.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{print}, \\code{predict}, \\code{coef} and \\code{plot} methods,\n",
       "and the \\code{cv.glmnet} function.\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "\n",
       "# Gaussian\n",
       "x = matrix(rnorm(100 * 20), 100, 20)\n",
       "y = rnorm(100)\n",
       "fit1 = glmnet(x, y)\n",
       "print(fit1)\n",
       "coef(fit1, s = 0.01)  # extract coefficients at a single value of lambda\n",
       "predict(fit1, newx = x[1:10, ], s = c(0.01, 0.005))  # make predictions\n",
       "\n",
       "# Relaxed\n",
       "fit1r = glmnet(x, y, relax = TRUE)  # can be used with any model\n",
       "\n",
       "# multivariate gaussian\n",
       "y = matrix(rnorm(100 * 3), 100, 3)\n",
       "fit1m = glmnet(x, y, family = \"mgaussian\")\n",
       "plot(fit1m, type.coef = \"2norm\")\n",
       "\n",
       "# binomial\n",
       "g2 = sample(c(0,1), 100, replace = TRUE)\n",
       "fit2 = glmnet(x, g2, family = \"binomial\")\n",
       "fit2n = glmnet(x, g2, family = binomial(link=cloglog))\n",
       "fit2r = glmnet(x,g2, family = \"binomial\", relax=TRUE)\n",
       "fit2rp = glmnet(x,g2, family = \"binomial\", relax=TRUE, path=TRUE)\n",
       "\n",
       "# multinomial\n",
       "g4 = sample(1:4, 100, replace = TRUE)\n",
       "fit3 = glmnet(x, g4, family = \"multinomial\")\n",
       "fit3a = glmnet(x, g4, family = \"multinomial\", type.multinomial = \"grouped\")\n",
       "# poisson\n",
       "N = 500\n",
       "p = 20\n",
       "nzc = 5\n",
       "x = matrix(rnorm(N * p), N, p)\n",
       "beta = rnorm(nzc)\n",
       "f = x[, seq(nzc)] %*% beta\n",
       "mu = exp(f)\n",
       "y = rpois(N, mu)\n",
       "fit = glmnet(x, y, family = \"poisson\")\n",
       "plot(fit)\n",
       "pfit = predict(fit, x, s = 0.001, type = \"response\")\n",
       "plot(pfit, y)\n",
       "\n",
       "# Cox\n",
       "set.seed(10101)\n",
       "N = 1000\n",
       "p = 30\n",
       "nzc = p/3\n",
       "x = matrix(rnorm(N * p), N, p)\n",
       "beta = rnorm(nzc)\n",
       "fx = x[, seq(nzc)] %*% beta/3\n",
       "hx = exp(fx)\n",
       "ty = rexp(N, hx)\n",
       "tcens = rbinom(n = N, prob = 0.3, size = 1)  # censoring indicator\n",
       "y = cbind(time = ty, status = 1 - tcens)  # y=Surv(ty,1-tcens) with library(survival)\n",
       "fit = glmnet(x, y, family = \"cox\")\n",
       "plot(fit)\n",
       "\n",
       "# Cox example with (start, stop] data\n",
       "set.seed(2)\n",
       "nobs <- 100; nvars <- 15\n",
       "xvec <- rnorm(nobs * nvars)\n",
       "xvec[sample.int(nobs * nvars, size = 0.4 * nobs * nvars)] <- 0\n",
       "x <- matrix(xvec, nrow = nobs)\n",
       "start_time <- runif(100, min = 0, max = 5)\n",
       "stop_time <- start_time + runif(100, min = 0.1, max = 3)\n",
       "status <- rbinom(n = nobs, prob = 0.3, size = 1)\n",
       "jsurv_ss <- survival::Surv(start_time, stop_time, status)\n",
       "fit <- glmnet(x, jsurv_ss, family = \"cox\")\n",
       "\n",
       "# Cox example with strata\n",
       "jsurv_ss2 <- stratifySurv(jsurv_ss, rep(1:2, each = 50))\n",
       "fit <- glmnet(x, jsurv_ss2, family = \"cox\")\n",
       "\n",
       "# Sparse\n",
       "n = 10000\n",
       "p = 200\n",
       "nzc = trunc(p/10)\n",
       "x = matrix(rnorm(n * p), n, p)\n",
       "iz = sample(1:(n * p), size = n * p * 0.85, replace = FALSE)\n",
       "x[iz] = 0\n",
       "sx = Matrix(x, sparse = TRUE)\n",
       "inherits(sx, \"sparseMatrix\")  #confirm that it is sparse\n",
       "beta = rnorm(nzc)\n",
       "fx = x[, seq(nzc)] %*% beta\n",
       "eps = rnorm(n)\n",
       "y = fx + eps\n",
       "px = exp(fx)\n",
       "px = px/(1 + px)\n",
       "ly = rbinom(n = length(px), prob = px, size = 1)\n",
       "system.time(fit1 <- glmnet(sx, y))\n",
       "system.time(fit2n <- glmnet(x, y))\n",
       "\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "glmnet                 package:glmnet                  R Documentation\n",
       "\n",
       "_\bf_\bi_\bt _\ba _\bG_\bL_\bM _\bw_\bi_\bt_\bh _\bl_\ba_\bs_\bs_\bo _\bo_\br _\be_\bl_\ba_\bs_\bt_\bi_\bc_\bn_\be_\bt _\br_\be_\bg_\bu_\bl_\ba_\br_\bi_\bz_\ba_\bt_\bi_\bo_\bn\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     Fit a generalized linear model via penalized maximum likelihood.\n",
       "     The regularization path is computed for the lasso or elasticnet\n",
       "     penalty at a grid of values for the regularization parameter\n",
       "     lambda. Can deal with all shapes of data, including very large\n",
       "     sparse data matrices. Fits linear, logistic and multinomial,\n",
       "     poisson, and Cox regression models.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     glmnet(\n",
       "       x,\n",
       "       y,\n",
       "       family = c(\"gaussian\", \"binomial\", \"poisson\", \"multinomial\", \"cox\", \"mgaussian\"),\n",
       "       weights = NULL,\n",
       "       offset = NULL,\n",
       "       alpha = 1,\n",
       "       nlambda = 100,\n",
       "       lambda.min.ratio = ifelse(nobs < nvars, 0.01, 1e-04),\n",
       "       lambda = NULL,\n",
       "       standardize = TRUE,\n",
       "       intercept = TRUE,\n",
       "       thresh = 1e-07,\n",
       "       dfmax = nvars + 1,\n",
       "       pmax = min(dfmax * 2 + 20, nvars),\n",
       "       exclude = NULL,\n",
       "       penalty.factor = rep(1, nvars),\n",
       "       lower.limits = -Inf,\n",
       "       upper.limits = Inf,\n",
       "       maxit = 1e+05,\n",
       "       type.gaussian = ifelse(nvars < 500, \"covariance\", \"naive\"),\n",
       "       type.logistic = c(\"Newton\", \"modified.Newton\"),\n",
       "       standardize.response = FALSE,\n",
       "       type.multinomial = c(\"ungrouped\", \"grouped\"),\n",
       "       relax = FALSE,\n",
       "       trace.it = 0,\n",
       "       ...\n",
       "     )\n",
       "     \n",
       "     relax.glmnet(fit, x, ..., maxp = n - 3, path = FALSE, check.args = TRUE)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       "       x: input matrix, of dimension nobs x nvars; each row is an\n",
       "          observation vector. Can be in sparse matrix format (inherit\n",
       "          from class '\"sparseMatrix\"' as in package 'Matrix')\n",
       "\n",
       "       y: response variable. Quantitative for 'family=\"gaussian\"', or\n",
       "          'family=\"poisson\"' (non-negative counts). For\n",
       "          'family=\"binomial\"' should be either a factor with two\n",
       "          levels, or a two-column matrix of counts or proportions (the\n",
       "          second column is treated as the target class; for a factor,\n",
       "          the last level in alphabetical order is the target class).\n",
       "          For 'family=\"multinomial\"', can be a 'nc>=2' level factor, or\n",
       "          a matrix with 'nc' columns of counts or proportions. For\n",
       "          either '\"binomial\"' or '\"multinomial\"', if 'y' is presented\n",
       "          as a vector, it will be coerced into a factor. For\n",
       "          'family=\"cox\"', preferably a 'Surv' object from the survival\n",
       "          package: see Details section for more information. For\n",
       "          'family=\"mgaussian\"', 'y' is a matrix of quantitative\n",
       "          responses.\n",
       "\n",
       "  family: Either a character string representing one of the built-in\n",
       "          families, or else a 'glm()' family object. For more\n",
       "          information, see Details section below or the documentation\n",
       "          for response type (above).\n",
       "\n",
       " weights: observation weights. Can be total counts if responses are\n",
       "          proportion matrices. Default is 1 for each observation\n",
       "\n",
       "  offset: A vector of length 'nobs' that is included in the linear\n",
       "          predictor (a 'nobs x nc' matrix for the '\"multinomial\"'\n",
       "          family). Useful for the '\"poisson\"' family (e.g. log of\n",
       "          exposure time), or for refining a model by starting at a\n",
       "          current fit. Default is 'NULL'. If supplied, then values must\n",
       "          also be supplied to the 'predict' function.\n",
       "\n",
       "   alpha: The elasticnet mixing parameter, with 0<=alpha<= 1. The\n",
       "          penalty is defined as\n",
       "\n",
       "                     (1-alpha)/2||beta||_2^2+alpha||beta||_1.           \n",
       "          \n",
       "          'alpha=1' is the lasso penalty, and 'alpha=0' the ridge\n",
       "          penalty.\n",
       "\n",
       " nlambda: The number of 'lambda' values - default is 100.\n",
       "\n",
       "lambda.min.ratio: Smallest value for 'lambda', as a fraction of\n",
       "          'lambda.max', the (data derived) entry value (i.e. the\n",
       "          smallest value for which all coefficients are zero). The\n",
       "          default depends on the sample size 'nobs' relative to the\n",
       "          number of variables 'nvars'. If 'nobs > nvars', the default\n",
       "          is '0.0001', close to zero.  If 'nobs < nvars', the default\n",
       "          is '0.01'.  A very small value of 'lambda.min.ratio' will\n",
       "          lead to a saturated fit in the 'nobs < nvars' case. This is\n",
       "          undefined for '\"binomial\"' and '\"multinomial\"' models, and\n",
       "          'glmnet' will exit gracefully when the percentage deviance\n",
       "          explained is almost 1.\n",
       "\n",
       "  lambda: A user supplied 'lambda' sequence. Typical usage is to have\n",
       "          the program compute its own 'lambda' sequence based on\n",
       "          'nlambda' and 'lambda.min.ratio'. Supplying a value of\n",
       "          'lambda' overrides this. WARNING: use with care. Avoid\n",
       "          supplying a single value for 'lambda' (for predictions after\n",
       "          CV use 'predict()' instead).  Supply instead a decreasing\n",
       "          sequence of 'lambda' values. 'glmnet' relies on its warms\n",
       "          starts for speed, and its often faster to fit a whole path\n",
       "          than compute a single fit.\n",
       "\n",
       "standardize: Logical flag for x variable standardization, prior to\n",
       "          fitting the model sequence. The coefficients are always\n",
       "          returned on the original scale. Default is\n",
       "          'standardize=TRUE'.  If variables are in the same units\n",
       "          already, you might not wish to standardize. See details below\n",
       "          for y standardization with 'family=\"gaussian\"'.\n",
       "\n",
       "intercept: Should intercept(s) be fitted (default=TRUE) or set to zero\n",
       "          (FALSE)\n",
       "\n",
       "  thresh: Convergence threshold for coordinate descent. Each inner\n",
       "          coordinate-descent loop continues until the maximum change in\n",
       "          the objective after any coefficient update is less than\n",
       "          'thresh' times the null deviance. Defaults value is '1E-7'.\n",
       "\n",
       "   dfmax: Limit the maximum number of variables in the model. Useful\n",
       "          for very large 'nvars', if a partial path is desired.\n",
       "\n",
       "    pmax: Limit the maximum number of variables ever to be nonzero\n",
       "\n",
       " exclude: Indices of variables to be excluded from the model. Default\n",
       "          is none. Equivalent to an infinite penalty factor (next\n",
       "          item).\n",
       "\n",
       "penalty.factor: Separate penalty factors can be applied to each\n",
       "          coefficient. This is a number that multiplies 'lambda' to\n",
       "          allow differential shrinkage. Can be 0 for some variables,\n",
       "          which implies no shrinkage, and that variable is always\n",
       "          included in the model. Default is 1 for all variables (and\n",
       "          implicitly infinity for variables listed in 'exclude'). Note:\n",
       "          the penalty factors are internally rescaled to sum to nvars,\n",
       "          and the lambda sequence will reflect this change.\n",
       "\n",
       "lower.limits: Vector of lower limits for each coefficient; default\n",
       "          '-Inf'. Each of these must be non-positive. Can be presented\n",
       "          as a single value (which will then be replicated), else a\n",
       "          vector of length 'nvars'\n",
       "\n",
       "upper.limits: Vector of upper limits for each coefficient; default\n",
       "          'Inf'. See 'lower.limits'\n",
       "\n",
       "   maxit: Maximum number of passes over the data for all lambda values;\n",
       "          default is 10^5.\n",
       "\n",
       "type.gaussian: Two algorithm types are supported for (only)\n",
       "          'family=\"gaussian\"'. The default when 'nvar<500' is\n",
       "          'type.gaussian=\"covariance\"', and saves all inner-products\n",
       "          ever computed. This can be much faster than\n",
       "          'type.gaussian=\"naive\"', which loops through 'nobs' every\n",
       "          time an inner-product is computed. The latter can be far more\n",
       "          efficient for 'nvar >> nobs' situations, or when 'nvar >\n",
       "          500'.\n",
       "\n",
       "type.logistic: If '\"Newton\"' then the exact hessian is used (default),\n",
       "          while '\"modified.Newton\"' uses an upper-bound on the hessian,\n",
       "          and can be faster.\n",
       "\n",
       "standardize.response: This is for the 'family=\"mgaussian\"' family, and\n",
       "          allows the user to standardize the response variables\n",
       "\n",
       "type.multinomial: If '\"grouped\"' then a grouped lasso penalty is used\n",
       "          on the multinomial coefficients for a variable. This ensures\n",
       "          they are all in our out together. The default is\n",
       "          '\"ungrouped\"'\n",
       "\n",
       "   relax: If 'TRUE' then for each _active set_ in the path of\n",
       "          solutions, the model is refit without any regularization. See\n",
       "          'details' for more information. This argument is new, and\n",
       "          users may experience convergence issues with small datasets,\n",
       "          especially with non-gaussian families. Limiting the value of\n",
       "          'maxp' can alleviate these issues in some cases.\n",
       "\n",
       "trace.it: If 'trace.it=1', then a progress bar is displayed; useful for\n",
       "          big models that take a long time to fit.\n",
       "\n",
       "     ...: Additional argument used in 'relax.glmnet'. These include\n",
       "          some of the original arguments to 'glmnet', and each must be\n",
       "          named if used.\n",
       "\n",
       "     fit: For 'relax.glmnet' a fitted 'glmnet' object\n",
       "\n",
       "    maxp: a limit on how many relaxed coefficients are allowed. Default\n",
       "          is 'n-3', where 'n' is the sample size. This may not be\n",
       "          sufficient for non-gaussian familes, in which case users\n",
       "          should supply a smaller value. This argument can be supplied\n",
       "          directly to 'glmnet'.\n",
       "\n",
       "    path: Since 'glmnet' does not do stepsize optimization, the Newton\n",
       "          algorithm can get stuck and not converge, especially with\n",
       "          relaxed fits. With 'path=TRUE', each relaxed fit on a\n",
       "          particular set of variables is computed pathwise using the\n",
       "          original sequence of lambda values (with a zero attached to\n",
       "          the end). Not needed for Gaussian models, and should not be\n",
       "          used unless needed, since will lead to longer compute times.\n",
       "          Default is 'path=FALSE'. appropriate subset of variables\n",
       "\n",
       "check.args: Should 'relax.glmnet' make sure that all the data dependent\n",
       "          arguments used in creating 'fit' have been resupplied.\n",
       "          Default is 'TRUE'.\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     The sequence of models implied by 'lambda' is fit by coordinate\n",
       "     descent. For 'family=\"gaussian\"' this is the lasso sequence if\n",
       "     'alpha=1', else it is the elasticnet sequence.\n",
       "\n",
       "     The objective function for '\"gaussian\"' is\n",
       "\n",
       "                        1/2 RSS/nobs +lambda*penalty,                   \n",
       "     \n",
       "     and for the other models it is\n",
       "\n",
       "                        -loglik/nobs +lambda*penalty.                   \n",
       "     \n",
       "     Note also that for '\"gaussian\"', 'glmnet' standardizes y to have\n",
       "     unit variance (using 1/n rather than 1/(n-1) formula) before\n",
       "     computing its lambda sequence (and then unstandardizes the\n",
       "     resulting coefficients); if you wish to reproduce/compare results\n",
       "     with other software, best to supply a standardized y. The\n",
       "     coefficients for any predictor variables with zero variance are\n",
       "     set to zero for all values of lambda.\n",
       "\n",
       "  _\bD_\be_\bt_\ba_\bi_\bl_\bs _\bo_\bn '_\bf_\ba_\bm_\bi_\bl_\by' _\bo_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "       From version 4.0 onwards, glmnet supports both the original\n",
       "       built-in families, as well as _any_ family object as used by\n",
       "       'stats:glm()'. This opens the door to a wide variety of\n",
       "       additional models. For example 'family=binomial(link=cloglog)'\n",
       "       or 'family=negative.binomial(theta=1.5)' (from the MASS\n",
       "       library). Note that the code runs faster for the built-in\n",
       "       families.\n",
       "\n",
       "       The built in families are specifed via a character string. For\n",
       "       all families, the object produced is a lasso or elasticnet\n",
       "       regularization path for fitting the generalized linear\n",
       "       regression paths, by maximizing the appropriate penalized\n",
       "       log-likelihood (partial likelihood for the \"cox\" model).\n",
       "       Sometimes the sequence is truncated before 'nlambda' values of\n",
       "       'lambda' have been used, because of instabilities in the inverse\n",
       "       link functions near a saturated fit.\n",
       "       'glmnet(...,family=\"binomial\")' fits a traditional logistic\n",
       "       regression model for the log-odds.\n",
       "       'glmnet(...,family=\"multinomial\")' fits a symmetric multinomial\n",
       "       model, where each class is represented by a linear model (on the\n",
       "       log-scale). The penalties take care of redundancies. A two-class\n",
       "       '\"multinomial\"' model will produce the same fit as the\n",
       "       corresponding '\"binomial\"' model, except the pair of coefficient\n",
       "       matrices will be equal in magnitude and opposite in sign, and\n",
       "       half the '\"binomial\"' values. Two useful additional families are\n",
       "       the 'family=\"mgaussian\"' family and the\n",
       "       'type.multinomial=\"grouped\"' option for multinomial fitting. The\n",
       "       former allows a multi-response gaussian model to be fit, using a\n",
       "       \"group -lasso\" penalty on the coefficients for each variable.\n",
       "       Tying the responses together like this is called \"multi-task\"\n",
       "       learning in some domains. The grouped multinomial allows the\n",
       "       same penalty for the 'family=\"multinomial\"' model, which is also\n",
       "       multi-responsed. For both of these the penalty on the\n",
       "       coefficient vector for variable j is\n",
       "\n",
       "                 (1-alpha)/2||beta_j||_2^2+alpha||beta_j||_2.           \n",
       "       \n",
       "       When 'alpha=1' this is a group-lasso penalty, and otherwise it\n",
       "       mixes with quadratic just like elasticnet. A small detail in the\n",
       "       Cox model: if death times are tied with censored times, we\n",
       "       assume the censored times occurred just _before_ the death times\n",
       "       in computing the Breslow approximation; if users prefer the\n",
       "       usual convention of _after_, they can add a small number to all\n",
       "       censoring times to achieve this effect.\n",
       "\n",
       "\n",
       "  _\bD_\be_\bt_\ba_\bi_\bl_\bs _\bo_\bn _\br_\be_\bs_\bp_\bo_\bn_\bs_\be _\bf_\bo_\br '_\bf_\ba_\bm_\bi_\bl_\by=\"_\bc_\bo_\bx\"':\n",
       "\n",
       "       For Cox models, the response should preferably be a 'Surv'\n",
       "       object, created by the 'Surv()' function in 'survival' package.\n",
       "       For right-censored data, this object should have type \"right\",\n",
       "       and for (start, stop] data, it should have type \"counting\". To\n",
       "       fit stratified Cox models, strata should be added to the\n",
       "       response via the 'stratifySurv()' function before passing the\n",
       "       response to 'glmnet()'. (For backward compatibility,\n",
       "       right-censored data can also be passed as a two-column matrix\n",
       "       with columns named 'time' and 'status'. The latter is a binary\n",
       "       variable, with '1' indicating death, and '0' indicating right\n",
       "       censored.)\n",
       "\n",
       "\n",
       "  _\bD_\be_\bt_\ba_\bi_\bl_\bs _\bo_\bn '_\br_\be_\bl_\ba_\bx' _\bo_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "       If 'relax=TRUE' a duplicate sequence of models is produced,\n",
       "       where each active set in the elastic-net path is refit without\n",
       "       regularization. The result of this is a matching '\"glmnet\"'\n",
       "       object which is stored on the original object in a component\n",
       "       named '\"relaxed\"', and is part of the glmnet output. Generally\n",
       "       users will not call 'relax.glmnet' directly, unless the original\n",
       "       'glmnet' object took a long time to fit. But if they do, they\n",
       "       must supply the fit, and all the original arguments used to\n",
       "       create that fit. They can limit the length of the relaxed path\n",
       "       via 'maxp'.\n",
       "\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     An object with S3 class '\"glmnet\",\"*\" ', where '\"*\"' is '\"elnet\"',\n",
       "     '\"lognet\"', '\"multnet\"', '\"fishnet\"' (poisson), '\"coxnet\"' or\n",
       "     '\"mrelnet\"' for the various types of models. If the model was\n",
       "     created with 'relax=TRUE' then this class has a prefix class of\n",
       "     '\"relaxed\"'.\n",
       "\n",
       "    call: the call that produced this object\n",
       "\n",
       "      a0: Intercept sequence of length 'length(lambda)'\n",
       "\n",
       "    beta: For '\"elnet\"', '\"lognet\"', '\"fishnet\"' and '\"coxnet\"' models,\n",
       "          a 'nvars x length(lambda)' matrix of coefficients, stored in\n",
       "          sparse column format ('\"CsparseMatrix\"'). For '\"multnet\"' and\n",
       "          '\"mgaussian\"', a list of 'nc' such matrices, one for each\n",
       "          class.\n",
       "\n",
       "  lambda: The actual sequence of 'lambda' values used. When 'alpha=0',\n",
       "          the largest lambda reported does not quite give the zero\n",
       "          coefficients reported ('lambda=inf' would in principle).\n",
       "          Instead, the largest 'lambda' for 'alpha=0.001' is used, and\n",
       "          the sequence of 'lambda' values is derived from this.\n",
       "\n",
       "dev.ratio: The fraction of (null) deviance explained (for '\"elnet\"',\n",
       "          this is the R-square). The deviance calculations incorporate\n",
       "          weights if present in the model. The deviance is defined to\n",
       "          be 2*(loglike_sat - loglike), where loglike_sat is the\n",
       "          log-likelihood for the saturated model (a model with a free\n",
       "          parameter per observation). Hence dev.ratio=1-dev/nulldev.\n",
       "\n",
       " nulldev: Null deviance (per observation). This is defined to be\n",
       "          2*(loglike_sat -loglike(Null)); The NULL model refers to the\n",
       "          intercept model, except for the Cox, where it is the 0 model.\n",
       "\n",
       "      df: The number of nonzero coefficients for each value of\n",
       "          'lambda'. For '\"multnet\"', this is the number of variables\n",
       "          with a nonzero coefficient for _any_ class.\n",
       "\n",
       "   dfmat: For '\"multnet\"' and '\"mrelnet\"' only. A matrix consisting of\n",
       "          the number of nonzero coefficients per class\n",
       "\n",
       "     dim: dimension of coefficient matrix (ices)\n",
       "\n",
       "    nobs: number of observations\n",
       "\n",
       " npasses: total passes over the data summed over all lambda values\n",
       "\n",
       "  offset: a logical variable indicating whether an offset was included\n",
       "          in the model\n",
       "\n",
       "    jerr: error flag, for warnings and errors (largely for internal\n",
       "          debugging).\n",
       "\n",
       " relaxed: If 'relax=TRUE', this additional item is another glmnet\n",
       "          object with different values for 'beta' and 'dev.ratio'\n",
       "\n",
       "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
       "\n",
       "     Jerome Friedman, Trevor Hastie, Balasubramanian Narasimhan, Noah\n",
       "     Simon, Kenneth Tay and Rob Tibshirani\n",
       "     Maintainer: Trevor Hastie <email: hastie@stanford.edu>\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Friedman, J., Hastie, T. and Tibshirani, R. (2008) _Regularization\n",
       "     Paths for Generalized Linear Models via Coordinate Descent (2010),\n",
       "     Journal of Statistical Software, Vol. 33(1), 1-22_, <URL:\n",
       "     https://web.stanford.edu/~hastie/Papers/glmnet.pdf>.\n",
       "     Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2011)\n",
       "     _Regularization Paths for Cox's Proportional Hazards Model via\n",
       "     Coordinate Descent, Journal of Statistical Software, Vol. 39(5),\n",
       "     1-13_, <URL: https://www.jstatsoft.org/v39/i05/>.\n",
       "     Tibshirani, Robert, Bien, J., Friedman, J., Hastie, T.,Simon,\n",
       "     N.,Taylor, J. and Tibshirani, Ryan. (2012) _Strong Rules for\n",
       "     Discarding Predictors in Lasso-type Problems, JRSSB, Vol. 74(2),\n",
       "     245-266_, <URL:\n",
       "     https://statweb.stanford.edu/~tibs/ftp/strong.pdf>.\n",
       "     Hastie, T., Tibshirani, Robert and Tibshirani, Ryan. _Extended\n",
       "     Comparisons of Best Subset Selection, Forward Stepwise Selection,\n",
       "     and the Lasso (2017), Stanford Statistics Technical Report_, <URL:\n",
       "     https://arxiv.org/abs/1707.08692>.\n",
       "     Glmnet webpage with four vignettes, <URL:\n",
       "     https://glmnet.stanford.edu>.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     'print', 'predict', 'coef' and 'plot' methods, and the 'cv.glmnet'\n",
       "     function.\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     # Gaussian\n",
       "     x = matrix(rnorm(100 * 20), 100, 20)\n",
       "     y = rnorm(100)\n",
       "     fit1 = glmnet(x, y)\n",
       "     print(fit1)\n",
       "     coef(fit1, s = 0.01)  # extract coefficients at a single value of lambda\n",
       "     predict(fit1, newx = x[1:10, ], s = c(0.01, 0.005))  # make predictions\n",
       "     \n",
       "     # Relaxed\n",
       "     fit1r = glmnet(x, y, relax = TRUE)  # can be used with any model\n",
       "     \n",
       "     # multivariate gaussian\n",
       "     y = matrix(rnorm(100 * 3), 100, 3)\n",
       "     fit1m = glmnet(x, y, family = \"mgaussian\")\n",
       "     plot(fit1m, type.coef = \"2norm\")\n",
       "     \n",
       "     # binomial\n",
       "     g2 = sample(c(0,1), 100, replace = TRUE)\n",
       "     fit2 = glmnet(x, g2, family = \"binomial\")\n",
       "     fit2n = glmnet(x, g2, family = binomial(link=cloglog))\n",
       "     fit2r = glmnet(x,g2, family = \"binomial\", relax=TRUE)\n",
       "     fit2rp = glmnet(x,g2, family = \"binomial\", relax=TRUE, path=TRUE)\n",
       "     \n",
       "     # multinomial\n",
       "     g4 = sample(1:4, 100, replace = TRUE)\n",
       "     fit3 = glmnet(x, g4, family = \"multinomial\")\n",
       "     fit3a = glmnet(x, g4, family = \"multinomial\", type.multinomial = \"grouped\")\n",
       "     # poisson\n",
       "     N = 500\n",
       "     p = 20\n",
       "     nzc = 5\n",
       "     x = matrix(rnorm(N * p), N, p)\n",
       "     beta = rnorm(nzc)\n",
       "     f = x[, seq(nzc)] %*% beta\n",
       "     mu = exp(f)\n",
       "     y = rpois(N, mu)\n",
       "     fit = glmnet(x, y, family = \"poisson\")\n",
       "     plot(fit)\n",
       "     pfit = predict(fit, x, s = 0.001, type = \"response\")\n",
       "     plot(pfit, y)\n",
       "     \n",
       "     # Cox\n",
       "     set.seed(10101)\n",
       "     N = 1000\n",
       "     p = 30\n",
       "     nzc = p/3\n",
       "     x = matrix(rnorm(N * p), N, p)\n",
       "     beta = rnorm(nzc)\n",
       "     fx = x[, seq(nzc)] %*% beta/3\n",
       "     hx = exp(fx)\n",
       "     ty = rexp(N, hx)\n",
       "     tcens = rbinom(n = N, prob = 0.3, size = 1)  # censoring indicator\n",
       "     y = cbind(time = ty, status = 1 - tcens)  # y=Surv(ty,1-tcens) with library(survival)\n",
       "     fit = glmnet(x, y, family = \"cox\")\n",
       "     plot(fit)\n",
       "     \n",
       "     # Cox example with (start, stop] data\n",
       "     set.seed(2)\n",
       "     nobs <- 100; nvars <- 15\n",
       "     xvec <- rnorm(nobs * nvars)\n",
       "     xvec[sample.int(nobs * nvars, size = 0.4 * nobs * nvars)] <- 0\n",
       "     x <- matrix(xvec, nrow = nobs)\n",
       "     start_time <- runif(100, min = 0, max = 5)\n",
       "     stop_time <- start_time + runif(100, min = 0.1, max = 3)\n",
       "     status <- rbinom(n = nobs, prob = 0.3, size = 1)\n",
       "     jsurv_ss <- survival::Surv(start_time, stop_time, status)\n",
       "     fit <- glmnet(x, jsurv_ss, family = \"cox\")\n",
       "     \n",
       "     # Cox example with strata\n",
       "     jsurv_ss2 <- stratifySurv(jsurv_ss, rep(1:2, each = 50))\n",
       "     fit <- glmnet(x, jsurv_ss2, family = \"cox\")\n",
       "     \n",
       "     # Sparse\n",
       "     n = 10000\n",
       "     p = 200\n",
       "     nzc = trunc(p/10)\n",
       "     x = matrix(rnorm(n * p), n, p)\n",
       "     iz = sample(1:(n * p), size = n * p * 0.85, replace = FALSE)\n",
       "     x[iz] = 0\n",
       "     sx = Matrix(x, sparse = TRUE)\n",
       "     inherits(sx, \"sparseMatrix\")  #confirm that it is sparse\n",
       "     beta = rnorm(nzc)\n",
       "     fx = x[, seq(nzc)] %*% beta\n",
       "     eps = rnorm(n)\n",
       "     y = fx + eps\n",
       "     px = exp(fx)\n",
       "     px = px/(1 + px)\n",
       "     ly = rbinom(n = length(px), prob = px, size = 1)\n",
       "     system.time(fit1 <- glmnet(sx, y))\n",
       "     system.time(fit2n <- glmnet(x, y))\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?glmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>2550</li><li>433</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2550\n",
       "\\item 433\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2550\n",
       "2. 433\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2550  433"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ted$views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.ridge = glmnet(x=X,y=log(y),family=\"gaussian\",alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:  glmnet(x = X, y = log(y), family = \"gaussian\", alpha = 0) \n",
       "\n",
       "     Df  %Dev  Lambda\n",
       "1   433  0.00 139.200\n",
       "2   433  0.91 126.800\n",
       "3   433  0.99 115.600\n",
       "4   433  1.09 105.300\n",
       "5   433  1.19  95.930\n",
       "6   433  1.30  87.410\n",
       "7   433  1.42  79.650\n",
       "8   433  1.55  72.570\n",
       "9   433  1.70  66.120\n",
       "10  433  1.85  60.250\n",
       "11  433  2.02  54.900\n",
       "12  433  2.20  50.020\n",
       "13  433  2.40  45.580\n",
       "14  433  2.61  41.530\n",
       "15  433  2.84  37.840\n",
       "16  433  3.09  34.480\n",
       "17  433  3.36  31.410\n",
       "18  433  3.65  28.620\n",
       "19  433  3.96  26.080\n",
       "20  433  4.29  23.760\n",
       "21  433  4.64  21.650\n",
       "22  433  5.02  19.730\n",
       "23  433  5.42  17.980\n",
       "24  433  5.85  16.380\n",
       "25  433  6.30  14.920\n",
       "26  433  6.78  13.600\n",
       "27  433  7.29  12.390\n",
       "28  433  7.82  11.290\n",
       "29  433  8.38  10.290\n",
       "30  433  8.96   9.373\n",
       "31  433  9.57   8.540\n",
       "32  433 10.21   7.781\n",
       "33  433 10.87   7.090\n",
       "34  433 11.55   6.460\n",
       "35  433 12.25   5.886\n",
       "36  433 12.96   5.363\n",
       "37  433 13.70   4.887\n",
       "38  433 14.45   4.453\n",
       "39  433 15.21   4.057\n",
       "40  433 15.99   3.697\n",
       "41  433 16.77   3.368\n",
       "42  433 17.55   3.069\n",
       "43  433 18.34   2.797\n",
       "44  433 19.13   2.548\n",
       "45  433 19.91   2.322\n",
       "46  433 20.69   2.115\n",
       "47  433 21.46   1.928\n",
       "48  433 22.22   1.756\n",
       "49  433 22.97   1.600\n",
       "50  433 23.71   1.458\n",
       "51  433 24.42   1.329\n",
       "52  433 25.12   1.211\n",
       "53  433 25.80   1.103\n",
       "54  433 26.45   1.005\n",
       "55  433 27.08   0.916\n",
       "56  433 27.69   0.834\n",
       "57  433 28.27   0.760\n",
       "58  433 28.82   0.693\n",
       "59  433 29.35   0.631\n",
       "60  433 29.85   0.575\n",
       "61  433 30.32   0.524\n",
       "62  433 30.77   0.478\n",
       "63  433 31.19   0.435\n",
       "64  433 31.58   0.396\n",
       "65  433 31.94   0.361\n",
       "66  433 32.28   0.329\n",
       "67  433 32.60   0.300\n",
       "68  433 32.89   0.273\n",
       "69  433 33.16   0.249\n",
       "70  433 33.41   0.227\n",
       "71  433 33.64   0.207\n",
       "72  433 33.84   0.188\n",
       "73  433 34.03   0.172\n",
       "74  433 34.20   0.156\n",
       "75  433 34.36   0.142\n",
       "76  433 34.50   0.130\n",
       "77  433 34.63   0.118\n",
       "78  433 34.74   0.108\n",
       "79  433 34.85   0.098\n",
       "80  433 34.94   0.089\n",
       "81  433 35.02   0.082\n",
       "82  433 35.10   0.074\n",
       "83  433 35.17   0.068\n",
       "84  433 35.23   0.062\n",
       "85  433 35.28   0.056\n",
       "86  433 35.33   0.051\n",
       "87  433 35.37   0.047\n",
       "88  433 35.41   0.043\n",
       "89  433 35.45   0.039\n",
       "90  433 35.48   0.035\n",
       "91  433 35.51   0.032\n",
       "92  433 35.53   0.029\n",
       "93  433 35.56   0.027\n",
       "94  433 35.58   0.024\n",
       "95  433 35.60   0.022\n",
       "96  433 35.61   0.020\n",
       "97  433 35.63   0.018\n",
       "98  433 35.64   0.017\n",
       "99  433 35.65   0.015\n",
       "100 433 35.66   0.014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit.ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>433</li><li>100</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 433\n",
       "\\item 100\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 433\n",
       "2. 100\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 433 100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(fit.ridge$beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "100"
      ],
      "text/latex": [
       "100"
      ],
      "text/markdown": [
       "100"
      ],
      "text/plain": [
       "[1] 100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(fit.ridge$lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>2550</li><li>433</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2550\n",
       "\\item 433\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2550\n",
       "2. 433\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2550  433"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAP1BMVEUAAAAil+Yo4uVNTU1h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\nAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplot(log(fit.ridge$lambda),t(fit.ridge$beta),type='l',xlab=\"log(lam)\",ylab=\"beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>1e-10</li><li>1.59228279334109e-10</li><li>2.53536449397012e-10</li><li>4.03701725859655e-10</li><li>6.42807311728432e-10</li><li>1.02353102189903e-09</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1e-10\n",
       "\\item 1.59228279334109e-10\n",
       "\\item 2.53536449397012e-10\n",
       "\\item 4.03701725859655e-10\n",
       "\\item 6.42807311728432e-10\n",
       "\\item 1.02353102189903e-09\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1e-10\n",
       "2. 1.59228279334109e-10\n",
       "3. 2.53536449397012e-10\n",
       "4. 4.03701725859655e-10\n",
       "5. 6.42807311728432e-10\n",
       "6. 1.02353102189903e-09\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1.000000e-10 1.592283e-10 2.535364e-10 4.037017e-10 6.428073e-10\n",
       "[6] 1.023531e-09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lam_seq = 10^seq(-10,10,length.out=100)\n",
    "head(lam_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.ridge = glmnet(x=X,y=log(y),family=\"gaussian\",alpha=0,lambda=lam_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAP1BMVEUAAAAil+Yo4uVNTU1h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\nQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplot(log(fit.ridge$lambda),t(fit.ridge$beta),type='l',xlab=\"log(lam)\",ylab=\"beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n"
     ]
    }
   ],
   "source": [
    "cv.ridge = cv.glmnet(x=X,y=log(y),family=\"gaussian\",alpha=0,lambda=lam_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAABNTU1oaGh8fHyM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\nKUmNBAAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(cv.ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.792482898353919"
      ],
      "text/latex": [
       "0.792482898353919"
      ],
      "text/markdown": [
       "0.792482898353919"
      ],
      "text/plain": [
       "[1] 0.7924829"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv.ridge$lambda.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "best.ridge = glmnet(x=X,y=log(y),family=\"gaussian\",alpha=0,lambda=cv.ridge$lambda.min)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd,R"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
