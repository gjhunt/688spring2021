{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 21 - Boosting\n",
    "# Lecture 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets work with spambase again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "library('bayesreg')\n",
    "data(spambase)\n",
    "colnames(spambase)[50:55] <- paste0('char.freq.',c('semic','paren','bracket','exclaim','dollar','pound'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'gbm' was built under R version 4.0.4\"\n",
      "Loaded gbm 2.1.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library('gbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for gbm {gbm}\"><tr><td>gbm {gbm}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Generalized Boosted Regression Modeling (GBM)</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>Fits generalized boosted regression models. For technical details, see the \n",
       "vignette: <code>utils::browseVignettes(\"gbm\")</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "gbm(\n",
       "  formula = formula(data),\n",
       "  distribution = \"bernoulli\",\n",
       "  data = list(),\n",
       "  weights,\n",
       "  var.monotone = NULL,\n",
       "  n.trees = 100,\n",
       "  interaction.depth = 1,\n",
       "  n.minobsinnode = 10,\n",
       "  shrinkage = 0.1,\n",
       "  bag.fraction = 0.5,\n",
       "  train.fraction = 1,\n",
       "  cv.folds = 0,\n",
       "  keep.data = TRUE,\n",
       "  verbose = FALSE,\n",
       "  class.stratify.cv = NULL,\n",
       "  n.cores = NULL\n",
       ")\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>formula</code></td>\n",
       "<td>\n",
       "<p>A symbolic description of the model to be fit. The formula\n",
       "may include an offset term (e.g. y~offset(n)+x). If \n",
       "<code>keep.data = FALSE</code> in the initial call to <code>gbm</code> then it is the \n",
       "user's responsibility to resupply the offset to <code>gbm.more</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>distribution</code></td>\n",
       "<td>\n",
       "<p>Either a character string specifying the name of the\n",
       "distribution to use or a list with a component <code>name</code> specifying the\n",
       "distribution and any additional parameters needed. If not specified,\n",
       "<code>gbm</code> will try to guess: if the response has only 2 unique values,\n",
       "bernoulli is assumed; otherwise, if the response is a factor, multinomial is\n",
       "assumed; otherwise, if the response has class <code>\"Surv\"</code>, coxph is \n",
       "assumed; otherwise, gaussian is assumed.\n",
       "</p>\n",
       "<p>Currently available options are <code>\"gaussian\"</code> (squared error), \n",
       "<code>\"laplace\"</code> (absolute loss), <code>\"tdist\"</code> (t-distribution loss), \n",
       "<code>\"bernoulli\"</code> (logistic regression for 0-1 outcomes), \n",
       "<code>\"huberized\"</code> (huberized hinge loss for 0-1 outcomes), classes), \n",
       "<code>\"adaboost\"</code> (the AdaBoost exponential loss for 0-1 outcomes),\n",
       "<code>\"poisson\"</code> (count outcomes), <code>\"coxph\"</code> (right censored \n",
       "observations), <code>\"quantile\"</code>, or <code>\"pairwise\"</code> (ranking measure \n",
       "using the LambdaMart algorithm).\n",
       "</p>\n",
       "<p>If quantile regression is specified, <code>distribution</code> must be a list of\n",
       "the form <code>list(name = \"quantile\", alpha = 0.25)</code> where <code>alpha</code> is \n",
       "the quantile to estimate. The current version's quantile regression method \n",
       "does not handle non-constant weights and will stop.\n",
       "</p>\n",
       "<p>If <code>\"tdist\"</code> is specified, the default degrees of freedom is 4 and \n",
       "this can be controlled by specifying \n",
       "<code>distribution = list(name = \"tdist\", df = DF)</code> where <code>DF</code> is your \n",
       "chosen degrees of freedom.\n",
       "</p>\n",
       "<p>If &quot;pairwise&quot; regression is specified, <code>distribution</code> must be a list of\n",
       "the form <code>list(name=\"pairwise\",group=...,metric=...,max.rank=...)</code>\n",
       "(<code>metric</code> and <code>max.rank</code> are optional, see below). <code>group</code> is\n",
       "a character vector with the column names of <code>data</code> that jointly\n",
       "indicate the group an instance belongs to (typically a query in Information\n",
       "Retrieval applications). For training, only pairs of instances from the same\n",
       "group and with different target labels can be considered. <code>metric</code> is\n",
       "the IR measure to use, one of \n",
       "</p>\n",
       " \n",
       "<dl>\n",
       "<dt>list(&quot;conc&quot;)</dt><dd><p>Fraction of concordant pairs; for binary labels, this \n",
       "is equivalent to the Area under the ROC Curve</p>\n",
       "</dd>\n",
       "<dt>:</dt><dd><p>Fraction of concordant pairs; for binary labels, this is \n",
       "equivalent to the Area under the ROC Curve</p>\n",
       "</dd> \n",
       "<dt>list(&quot;mrr&quot;)</dt><dd><p>Mean reciprocal rank of the highest-ranked positive \n",
       "instance</p>\n",
       "</dd>\n",
       "<dt>:</dt><dd><p>Mean reciprocal rank of the highest-ranked positive instance</p>\n",
       "</dd>\n",
       "<dt>list(&quot;map&quot;)</dt><dd><p>Mean average precision, a generalization of <code>mrr</code> \n",
       "to multiple positive instances</p>\n",
       "</dd><dt>:</dt><dd><p>Mean average precision, a\n",
       "generalization of <code>mrr</code> to multiple positive instances</p>\n",
       "</dd>\n",
       "<dt>list(&quot;ndcg:&quot;)</dt><dd><p>Normalized discounted cumulative gain. The score is \n",
       "the weighted sum (DCG) of the user-supplied target values, weighted \n",
       "by log(rank+1), and normalized to the maximum achievable value. This \n",
       "is the default if the user did not specify a metric.</p>\n",
       "</dd> \n",
       "</dl>\n",
       "\n",
       "<p><code>ndcg</code> and <code>conc</code> allow arbitrary target values, while binary\n",
       "targets 0,1 are expected for <code>map</code> and <code>mrr</code>. For <code>ndcg</code>\n",
       "and <code>mrr</code>, a cut-off can be chosen using a positive integer parameter\n",
       "<code>max.rank</code>. If left unspecified, all ranks are taken into account.\n",
       "</p>\n",
       "<p>Note that splitting of instances into training and validation sets follows\n",
       "group boundaries and therefore only approximates the specified\n",
       "<code>train.fraction</code> ratio (the same applies to cross-validation folds).\n",
       "Internally, queries are randomly shuffled before training, to avoid bias.\n",
       "</p>\n",
       "<p>Weights can be used in conjunction with pairwise metrics, however it is\n",
       "assumed that they are constant for instances from the same group.\n",
       "</p>\n",
       "<p>For details and background on the algorithm, see e.g. Burges (2010).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>data</code></td>\n",
       "<td>\n",
       "<p>an optional data frame containing the variables in the model. By\n",
       "default the variables are taken from <code>environment(formula)</code>, typically\n",
       "the environment from which <code>gbm</code> is called. If <code>keep.data=TRUE</code> in\n",
       "the initial call to <code>gbm</code> then <code>gbm</code> stores a copy with the\n",
       "object. If <code>keep.data=FALSE</code> then subsequent calls to\n",
       "<code>gbm.more</code> must resupply the same dataset. It becomes the user's\n",
       "responsibility to resupply the same data at this point.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>weights</code></td>\n",
       "<td>\n",
       "<p>an optional vector of weights to be used in the fitting\n",
       "process. Must be positive but do not need to be normalized. If\n",
       "<code>keep.data=FALSE</code> in the initial call to <code>gbm</code> then it is the\n",
       "user's responsibility to resupply the weights to <code>gbm.more</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>var.monotone</code></td>\n",
       "<td>\n",
       "<p>an optional vector, the same length as the number of\n",
       "predictors, indicating which variables have a monotone increasing (+1),\n",
       "decreasing (-1), or arbitrary (0) relationship with the outcome.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>n.trees</code></td>\n",
       "<td>\n",
       "<p>Integer specifying the total number of trees to fit. This is \n",
       "equivalent to the number of iterations and the number of basis functions in \n",
       "the additive expansion. Default is 100.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>interaction.depth</code></td>\n",
       "<td>\n",
       "<p>Integer specifying the maximum depth of each tree \n",
       "(i.e., the highest level of variable interactions allowed). A value of 1 \n",
       "implies an additive model, a value of 2 implies a model with up to 2-way \n",
       "interactions, etc. Default is 1.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>n.minobsinnode</code></td>\n",
       "<td>\n",
       "<p>Integer specifying the minimum number of observations \n",
       "in the terminal nodes of the trees. Note that this is the actual number of \n",
       "observations, not the total weight.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>shrinkage</code></td>\n",
       "<td>\n",
       "<p>a shrinkage parameter applied to each tree in the\n",
       "expansion. Also known as the learning rate or step-size reduction; 0.001 to \n",
       "0.1 usually work, but a smaller learning rate typically requires more trees.\n",
       "Default is 0.1.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>bag.fraction</code></td>\n",
       "<td>\n",
       "<p>the fraction of the training set observations randomly\n",
       "selected to propose the next tree in the expansion. This introduces\n",
       "randomnesses into the model fit. If <code>bag.fraction</code> &lt; 1 then running the\n",
       "same model twice will result in similar but different fits. <code>gbm</code> uses\n",
       "the R random number generator so <code>set.seed</code> can ensure that the model\n",
       "can be reconstructed. Preferably, the user can save the returned\n",
       "<code>gbm.object</code> using <code>save</code>. Default is 0.5.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>train.fraction</code></td>\n",
       "<td>\n",
       "<p>The first <code>train.fraction * nrows(data)</code>\n",
       "observations are used to fit the <code>gbm</code> and the remainder are used for\n",
       "computing out-of-sample estimates of the loss function.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>cv.folds</code></td>\n",
       "<td>\n",
       "<p>Number of cross-validation folds to perform. If\n",
       "<code>cv.folds</code>&gt;1 then <code>gbm</code>, in addition to the usual fit, will\n",
       "perform a cross-validation, calculate an estimate of generalization error\n",
       "returned in <code>cv.error</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>keep.data</code></td>\n",
       "<td>\n",
       "<p>a logical variable indicating whether to keep the data and\n",
       "an index of the data stored with the object. Keeping the data and index\n",
       "makes subsequent calls to <code>gbm.more</code> faster at the cost of\n",
       "storing an extra copy of the dataset.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>verbose</code></td>\n",
       "<td>\n",
       "<p>Logical indicating whether or not to print out progress and \n",
       "performance indicators (<code>TRUE</code>). If this option is left unspecified for \n",
       "<code>gbm.more</code>, then it uses <code>verbose</code> from <code>object</code>. Default is\n",
       "<code>FALSE</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>class.stratify.cv</code></td>\n",
       "<td>\n",
       "<p>Logical indicating whether or not the \n",
       "cross-validation should be stratified by class. Defaults to <code>TRUE</code> for\n",
       "<code>distribution = \"multinomial\"</code> and is only implemented for\n",
       "<code>\"multinomial\"</code> and <code>\"bernoulli\"</code>. The purpose of stratifying the\n",
       "cross-validation is to help avoiding situations in which training sets do\n",
       "not contain all classes.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>n.cores</code></td>\n",
       "<td>\n",
       "<p>The number of CPU cores to use. The cross-validation loop\n",
       "will attempt to send different CV folds off to different cores. If\n",
       "<code>n.cores</code> is not specified by the user, it is guessed using the\n",
       "<code>detectCores</code> function in the <code>parallel</code> package. Note that the\n",
       "documentation for <code>detectCores</code> makes clear that it is not failsafe and\n",
       "could return a spurious number of available cores.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p><code>gbm.fit</code> provides the link between R and the C++ gbm engine.\n",
       "<code>gbm</code> is a front-end to <code>gbm.fit</code> that uses the familiar R\n",
       "modeling formulas. However, <code>model.frame</code> is very slow if\n",
       "there are many predictor variables. For power-users with many variables use\n",
       "<code>gbm.fit</code>. For general practice <code>gbm</code> is preferable.\n",
       "</p>\n",
       "<p>This package implements the generalized boosted modeling framework. Boosting\n",
       "is the process of iteratively adding basis functions in a greedy fashion so\n",
       "that each additional basis function further reduces the selected loss\n",
       "function. This implementation closely follows Friedman's Gradient Boosting\n",
       "Machine (Friedman, 2001).\n",
       "</p>\n",
       "<p>In addition to many of the features documented in the Gradient Boosting\n",
       "Machine, <code>gbm</code> offers additional features including the out-of-bag\n",
       "estimator for the optimal number of iterations, the ability to store and\n",
       "manipulate the resulting <code>gbm</code> object, and a variety of other loss\n",
       "functions that had not previously had associated boosting algorithms,\n",
       "including the Cox partial likelihood for censored data, the poisson\n",
       "likelihood for count outcomes, and a gradient boosting implementation to\n",
       "minimize the AdaBoost exponential loss function.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>A <code>gbm.object</code> object.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Author(s)</h3>\n",
       "\n",
       "<p>Greg Ridgeway <a href=\"mailto:gregridgeway@gmail.com\">gregridgeway@gmail.com</a>\n",
       "</p>\n",
       "<p>Quantile regression code developed by Brian Kriegler\n",
       "<a href=\"mailto:bk@stat.ucla.edu\">bk@stat.ucla.edu</a>\n",
       "</p>\n",
       "<p>t-distribution, and multinomial code developed by Harry Southworth and\n",
       "Daniel Edwards\n",
       "</p>\n",
       "<p>Pairwise code developed by Stefan Schroedl <a href=\"mailto:schroedl@a9.com\">schroedl@a9.com</a>\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Y. Freund and R.E. Schapire (1997) &ldquo;A decision-theoretic\n",
       "generalization of on-line learning and an application to boosting,&rdquo;\n",
       "<em>Journal of Computer and System Sciences,</em> 55(1):119-139.\n",
       "</p>\n",
       "<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science\n",
       "and Statistics</em> 31:172-181.\n",
       "</p>\n",
       "<p>J.H. Friedman, T. Hastie, R. Tibshirani (2000). &ldquo;Additive Logistic\n",
       "Regression: a Statistical View of Boosting,&rdquo; <em>Annals of Statistics</em>\n",
       "28(2):337-374.\n",
       "</p>\n",
       "<p>J.H. Friedman (2001). &ldquo;Greedy Function Approximation: A Gradient\n",
       "Boosting Machine,&rdquo; <em>Annals of Statistics</em> 29(5):1189-1232.\n",
       "</p>\n",
       "<p>J.H. Friedman (2002). &ldquo;Stochastic Gradient Boosting,&rdquo;\n",
       "<em>Computational Statistics and Data Analysis</em> 38(4):367-378.\n",
       "</p>\n",
       "<p>B. Kriegler (2007). Cost-Sensitive Stochastic Gradient Boosting Within a \n",
       "Quantitative Regression Framework. Ph.D. Dissertation. University of \n",
       "California at Los Angeles, Los Angeles, CA, USA. Advisor(s) Richard A. Berk. \n",
       "urlhttps://dl.acm.org/citation.cfm?id=1354603.\n",
       "</p>\n",
       "<p>C. Burges (2010). &ldquo;From RankNet to LambdaRank to LambdaMART: An\n",
       "Overview,&rdquo; Microsoft Research Technical Report MSR-TR-2010-82.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>gbm.object</code>, <code>gbm.perf</code>, \n",
       "<code>plot.gbm</code>, <code>predict.gbm</code>, <code>summary.gbm</code>, \n",
       "and <code>pretty.gbm.tree</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "#\n",
       "# A least squares regression example \n",
       "#\n",
       "\n",
       "# Simulate data\n",
       "set.seed(101)  # for reproducibility\n",
       "N &lt;- 1000\n",
       "X1 &lt;- runif(N)\n",
       "X2 &lt;- 2 * runif(N)\n",
       "X3 &lt;- ordered(sample(letters[1:4], N, replace = TRUE), levels = letters[4:1])\n",
       "X4 &lt;- factor(sample(letters[1:6], N, replace = TRUE))\n",
       "X5 &lt;- factor(sample(letters[1:3], N, replace = TRUE))\n",
       "X6 &lt;- 3 * runif(N) \n",
       "mu &lt;- c(-1, 0, 1, 2)[as.numeric(X3)]\n",
       "SNR &lt;- 10  # signal-to-noise ratio\n",
       "Y &lt;- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu\n",
       "sigma &lt;- sqrt(var(Y) / SNR)\n",
       "Y &lt;- Y + rnorm(N, 0, sigma)\n",
       "X1[sample(1:N,size=500)] &lt;- NA  # introduce some missing values\n",
       "X4[sample(1:N,size=300)] &lt;- NA  # introduce some missing values\n",
       "data &lt;- data.frame(Y, X1, X2, X3, X4, X5, X6)\n",
       "\n",
       "# Fit a GBM\n",
       "set.seed(102)  # for reproducibility\n",
       "gbm1 &lt;- gbm(Y ~ ., data = data, var.monotone = c(0, 0, 0, 0, 0, 0),\n",
       "            distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1,             \n",
       "            interaction.depth = 3, bag.fraction = 0.5, train.fraction = 0.5,  \n",
       "            n.minobsinnode = 10, cv.folds = 5, keep.data = TRUE, \n",
       "            verbose = FALSE, n.cores = 1)  \n",
       "\n",
       "# Check performance using the out-of-bag (OOB) error; the OOB error typically\n",
       "# underestimates the optimal number of iterations\n",
       "best.iter &lt;- gbm.perf(gbm1, method = \"OOB\")\n",
       "print(best.iter)\n",
       "\n",
       "# Check performance using the 50% heldout test set\n",
       "best.iter &lt;- gbm.perf(gbm1, method = \"test\")\n",
       "print(best.iter)\n",
       "\n",
       "# Check performance using 5-fold cross-validation\n",
       "best.iter &lt;- gbm.perf(gbm1, method = \"cv\")\n",
       "print(best.iter)\n",
       "\n",
       "# Plot relative influence of each variable\n",
       "par(mfrow = c(1, 2))\n",
       "summary(gbm1, n.trees = 1)          # using first tree\n",
       "summary(gbm1, n.trees = best.iter)  # using estimated best number of trees\n",
       "\n",
       "# Compactly print the first and last trees for curiosity\n",
       "print(pretty.gbm.tree(gbm1, i.tree = 1))\n",
       "print(pretty.gbm.tree(gbm1, i.tree = gbm1$n.trees))\n",
       "\n",
       "# Simulate new data\n",
       "set.seed(103)  # for reproducibility\n",
       "N &lt;- 1000\n",
       "X1 &lt;- runif(N)\n",
       "X2 &lt;- 2 * runif(N)\n",
       "X3 &lt;- ordered(sample(letters[1:4], N, replace = TRUE))\n",
       "X4 &lt;- factor(sample(letters[1:6], N, replace = TRUE))\n",
       "X5 &lt;- factor(sample(letters[1:3], N, replace = TRUE))\n",
       "X6 &lt;- 3 * runif(N) \n",
       "mu &lt;- c(-1, 0, 1, 2)[as.numeric(X3)]\n",
       "Y &lt;- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu + rnorm(N, 0, sigma)\n",
       "data2 &lt;- data.frame(Y, X1, X2, X3, X4, X5, X6)\n",
       "\n",
       "# Predict on the new data using the \"best\" number of trees; by default,\n",
       "# predictions will be on the link scale\n",
       "Yhat &lt;- predict(gbm1, newdata = data2, n.trees = best.iter, type = \"link\")\n",
       "\n",
       "# least squares error\n",
       "print(sum((data2$Y - Yhat)^2))\n",
       "\n",
       "# Construct univariate partial dependence plots\n",
       "plot(gbm1, i.var = 1, n.trees = best.iter)\n",
       "plot(gbm1, i.var = 2, n.trees = best.iter)\n",
       "plot(gbm1, i.var = \"X3\", n.trees = best.iter)  # can use index or name\n",
       "\n",
       "# Construct bivariate partial dependence plots\n",
       "plot(gbm1, i.var = 1:2, n.trees = best.iter)\n",
       "plot(gbm1, i.var = c(\"X2\", \"X3\"), n.trees = best.iter)\n",
       "plot(gbm1, i.var = 3:4, n.trees = best.iter)\n",
       "\n",
       "# Construct trivariate partial dependence plots\n",
       "plot(gbm1, i.var = c(1, 2, 6), n.trees = best.iter, \n",
       "     continuous.resolution = 20)\n",
       "plot(gbm1, i.var = 1:3, n.trees = best.iter)\n",
       "plot(gbm1, i.var = 2:4, n.trees = best.iter)\n",
       "plot(gbm1, i.var = 3:5, n.trees = best.iter)\n",
       "\n",
       "# Add more (i.e., 100) boosting iterations to the ensemble\n",
       "gbm2 &lt;- gbm.more(gbm1, n.new.trees = 100, verbose = FALSE)\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>gbm</em> version 2.1.8 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{gbm}{Generalized Boosted Regression Modeling (GBM)}{gbm}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "Fits generalized boosted regression models. For technical details, see the \n",
       "vignette: \\code{utils::browseVignettes(\"gbm\")}.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "gbm(\n",
       "  formula = formula(data),\n",
       "  distribution = \"bernoulli\",\n",
       "  data = list(),\n",
       "  weights,\n",
       "  var.monotone = NULL,\n",
       "  n.trees = 100,\n",
       "  interaction.depth = 1,\n",
       "  n.minobsinnode = 10,\n",
       "  shrinkage = 0.1,\n",
       "  bag.fraction = 0.5,\n",
       "  train.fraction = 1,\n",
       "  cv.folds = 0,\n",
       "  keep.data = TRUE,\n",
       "  verbose = FALSE,\n",
       "  class.stratify.cv = NULL,\n",
       "  n.cores = NULL\n",
       ")\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{formula}] A symbolic description of the model to be fit. The formula\n",
       "may include an offset term (e.g. y\\textasciitilde{}offset(n)+x). If \n",
       "\\code{keep.data = FALSE} in the initial call to \\code{gbm} then it is the \n",
       "user's responsibility to resupply the offset to \\code{\\LinkA{gbm.more}{gbm.more}}.\n",
       "\n",
       "\\item[\\code{distribution}] Either a character string specifying the name of the\n",
       "distribution to use or a list with a component \\code{name} specifying the\n",
       "distribution and any additional parameters needed. If not specified,\n",
       "\\code{gbm} will try to guess: if the response has only 2 unique values,\n",
       "bernoulli is assumed; otherwise, if the response is a factor, multinomial is\n",
       "assumed; otherwise, if the response has class \\code{\"Surv\"}, coxph is \n",
       "assumed; otherwise, gaussian is assumed.\n",
       "\n",
       "Currently available options are \\code{\"gaussian\"} (squared error), \n",
       "\\code{\"laplace\"} (absolute loss), \\code{\"tdist\"} (t-distribution loss), \n",
       "\\code{\"bernoulli\"} (logistic regression for 0-1 outcomes), \n",
       "\\code{\"huberized\"} (huberized hinge loss for 0-1 outcomes), classes), \n",
       "\\code{\"adaboost\"} (the AdaBoost exponential loss for 0-1 outcomes),\n",
       "\\code{\"poisson\"} (count outcomes), \\code{\"coxph\"} (right censored \n",
       "observations), \\code{\"quantile\"}, or \\code{\"pairwise\"} (ranking measure \n",
       "using the LambdaMart algorithm).\n",
       "\n",
       "If quantile regression is specified, \\code{distribution} must be a list of\n",
       "the form \\code{list(name = \"quantile\", alpha = 0.25)} where \\code{alpha} is \n",
       "the quantile to estimate. The current version's quantile regression method \n",
       "does not handle non-constant weights and will stop.\n",
       "\n",
       "If \\code{\"tdist\"} is specified, the default degrees of freedom is 4 and \n",
       "this can be controlled by specifying \n",
       "\\code{distribution = list(name = \"tdist\", df = DF)} where \\code{DF} is your \n",
       "chosen degrees of freedom.\n",
       "\n",
       "If \"pairwise\" regression is specified, \\code{distribution} must be a list of\n",
       "the form \\code{list(name=\"pairwise\",group=...,metric=...,max.rank=...)}\n",
       "(\\code{metric} and \\code{max.rank} are optional, see below). \\code{group} is\n",
       "a character vector with the column names of \\code{data} that jointly\n",
       "indicate the group an instance belongs to (typically a query in Information\n",
       "Retrieval applications). For training, only pairs of instances from the same\n",
       "group and with different target labels can be considered. \\code{metric} is\n",
       "the IR measure to use, one of \n",
       "\\begin{description}\n",
       " \n",
       "\\item[list(\"conc\")] Fraction of concordant pairs; for binary labels, this \n",
       "is equivalent to the Area under the ROC Curve\n",
       "\\item[:] Fraction of concordant pairs; for binary labels, this is \n",
       "equivalent to the Area under the ROC Curve\n",
       "\\item[list(\"mrr\")] Mean reciprocal rank of the highest-ranked positive \n",
       "instance\n",
       "\\item[:] Mean reciprocal rank of the highest-ranked positive instance\n",
       "\\item[list(\"map\")] Mean average precision, a generalization of \\code{mrr} \n",
       "to multiple positive instances\\item[:] Mean average precision, a\n",
       "generalization of \\code{mrr} to multiple positive instances\n",
       "\\item[list(\"ndcg:\")] Normalized discounted cumulative gain. The score is \n",
       "the weighted sum (DCG) of the user-supplied target values, weighted \n",
       "by log(rank+1), and normalized to the maximum achievable value. This \n",
       "is the default if the user did not specify a metric.\n",
       "\n",
       "\\end{description}\n",
       "\n",
       "\n",
       "\\code{ndcg} and \\code{conc} allow arbitrary target values, while binary\n",
       "targets 0,1 are expected for \\code{map} and \\code{mrr}. For \\code{ndcg}\n",
       "and \\code{mrr}, a cut-off can be chosen using a positive integer parameter\n",
       "\\code{max.rank}. If left unspecified, all ranks are taken into account.\n",
       "\n",
       "Note that splitting of instances into training and validation sets follows\n",
       "group boundaries and therefore only approximates the specified\n",
       "\\code{train.fraction} ratio (the same applies to cross-validation folds).\n",
       "Internally, queries are randomly shuffled before training, to avoid bias.\n",
       "\n",
       "Weights can be used in conjunction with pairwise metrics, however it is\n",
       "assumed that they are constant for instances from the same group.\n",
       "\n",
       "For details and background on the algorithm, see e.g. Burges (2010).\n",
       "\n",
       "\\item[\\code{data}] an optional data frame containing the variables in the model. By\n",
       "default the variables are taken from \\code{environment(formula)}, typically\n",
       "the environment from which \\code{gbm} is called. If \\code{keep.data=TRUE} in\n",
       "the initial call to \\code{gbm} then \\code{gbm} stores a copy with the\n",
       "object. If \\code{keep.data=FALSE} then subsequent calls to\n",
       "\\code{\\LinkA{gbm.more}{gbm.more}} must resupply the same dataset. It becomes the user's\n",
       "responsibility to resupply the same data at this point.\n",
       "\n",
       "\\item[\\code{weights}] an optional vector of weights to be used in the fitting\n",
       "process. Must be positive but do not need to be normalized. If\n",
       "\\code{keep.data=FALSE} in the initial call to \\code{gbm} then it is the\n",
       "user's responsibility to resupply the weights to \\code{\\LinkA{gbm.more}{gbm.more}}.\n",
       "\n",
       "\\item[\\code{var.monotone}] an optional vector, the same length as the number of\n",
       "predictors, indicating which variables have a monotone increasing (+1),\n",
       "decreasing (-1), or arbitrary (0) relationship with the outcome.\n",
       "\n",
       "\\item[\\code{n.trees}] Integer specifying the total number of trees to fit. This is \n",
       "equivalent to the number of iterations and the number of basis functions in \n",
       "the additive expansion. Default is 100.\n",
       "\n",
       "\\item[\\code{interaction.depth}] Integer specifying the maximum depth of each tree \n",
       "(i.e., the highest level of variable interactions allowed). A value of 1 \n",
       "implies an additive model, a value of 2 implies a model with up to 2-way \n",
       "interactions, etc. Default is 1.\n",
       "\n",
       "\\item[\\code{n.minobsinnode}] Integer specifying the minimum number of observations \n",
       "in the terminal nodes of the trees. Note that this is the actual number of \n",
       "observations, not the total weight.\n",
       "\n",
       "\\item[\\code{shrinkage}] a shrinkage parameter applied to each tree in the\n",
       "expansion. Also known as the learning rate or step-size reduction; 0.001 to \n",
       "0.1 usually work, but a smaller learning rate typically requires more trees.\n",
       "Default is 0.1.\n",
       "\n",
       "\\item[\\code{bag.fraction}] the fraction of the training set observations randomly\n",
       "selected to propose the next tree in the expansion. This introduces\n",
       "randomnesses into the model fit. If \\code{bag.fraction} < 1 then running the\n",
       "same model twice will result in similar but different fits. \\code{gbm} uses\n",
       "the R random number generator so \\code{set.seed} can ensure that the model\n",
       "can be reconstructed. Preferably, the user can save the returned\n",
       "\\code{\\LinkA{gbm.object}{gbm.object}} using \\code{\\LinkA{save}{save}}. Default is 0.5.\n",
       "\n",
       "\\item[\\code{train.fraction}] The first \\code{train.fraction * nrows(data)}\n",
       "observations are used to fit the \\code{gbm} and the remainder are used for\n",
       "computing out-of-sample estimates of the loss function.\n",
       "\n",
       "\\item[\\code{cv.folds}] Number of cross-validation folds to perform. If\n",
       "\\code{cv.folds}>1 then \\code{gbm}, in addition to the usual fit, will\n",
       "perform a cross-validation, calculate an estimate of generalization error\n",
       "returned in \\code{cv.error}.\n",
       "\n",
       "\\item[\\code{keep.data}] a logical variable indicating whether to keep the data and\n",
       "an index of the data stored with the object. Keeping the data and index\n",
       "makes subsequent calls to \\code{\\LinkA{gbm.more}{gbm.more}} faster at the cost of\n",
       "storing an extra copy of the dataset.\n",
       "\n",
       "\\item[\\code{verbose}] Logical indicating whether or not to print out progress and \n",
       "performance indicators (\\code{TRUE}). If this option is left unspecified for \n",
       "\\code{gbm.more}, then it uses \\code{verbose} from \\code{object}. Default is\n",
       "\\code{FALSE}.\n",
       "\n",
       "\\item[\\code{class.stratify.cv}] Logical indicating whether or not the \n",
       "cross-validation should be stratified by class. Defaults to \\code{TRUE} for\n",
       "\\code{distribution = \"multinomial\"} and is only implemented for\n",
       "\\code{\"multinomial\"} and \\code{\"bernoulli\"}. The purpose of stratifying the\n",
       "cross-validation is to help avoiding situations in which training sets do\n",
       "not contain all classes.\n",
       "\n",
       "\\item[\\code{n.cores}] The number of CPU cores to use. The cross-validation loop\n",
       "will attempt to send different CV folds off to different cores. If\n",
       "\\code{n.cores} is not specified by the user, it is guessed using the\n",
       "\\code{detectCores} function in the \\code{parallel} package. Note that the\n",
       "documentation for \\code{detectCores} makes clear that it is not failsafe and\n",
       "could return a spurious number of available cores.\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "\\code{gbm.fit} provides the link between R and the C++ gbm engine.\n",
       "\\code{gbm} is a front-end to \\code{gbm.fit} that uses the familiar R\n",
       "modeling formulas. However, \\code{\\LinkA{model.frame}{model.frame}} is very slow if\n",
       "there are many predictor variables. For power-users with many variables use\n",
       "\\code{gbm.fit}. For general practice \\code{gbm} is preferable.\n",
       "\n",
       "\n",
       "This package implements the generalized boosted modeling framework. Boosting\n",
       "is the process of iteratively adding basis functions in a greedy fashion so\n",
       "that each additional basis function further reduces the selected loss\n",
       "function. This implementation closely follows Friedman's Gradient Boosting\n",
       "Machine (Friedman, 2001).\n",
       "\n",
       "In addition to many of the features documented in the Gradient Boosting\n",
       "Machine, \\code{gbm} offers additional features including the out-of-bag\n",
       "estimator for the optimal number of iterations, the ability to store and\n",
       "manipulate the resulting \\code{gbm} object, and a variety of other loss\n",
       "functions that had not previously had associated boosting algorithms,\n",
       "including the Cox partial likelihood for censored data, the poisson\n",
       "likelihood for count outcomes, and a gradient boosting implementation to\n",
       "minimize the AdaBoost exponential loss function.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "A \\code{\\LinkA{gbm.object}{gbm.object}} object.\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Author}\\relax\n",
       "Greg Ridgeway \\email{gregridgeway@gmail.com}\n",
       "\n",
       "Quantile regression code developed by Brian Kriegler\n",
       "\\email{bk@stat.ucla.edu}\n",
       "\n",
       "t-distribution, and multinomial code developed by Harry Southworth and\n",
       "Daniel Edwards\n",
       "\n",
       "Pairwise code developed by Stefan Schroedl \\email{schroedl@a9.com}\n",
       "\\end{Author}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Y. Freund and R.E. Schapire (1997) ``A decision-theoretic\n",
       "generalization of on-line learning and an application to boosting,''\n",
       "\\emph{Journal of Computer and System Sciences,} 55(1):119-139.\n",
       "\n",
       "G. Ridgeway (1999). ``The state of boosting,'' \\emph{Computing Science\n",
       "and Statistics} 31:172-181.\n",
       "\n",
       "J.H. Friedman, T. Hastie, R. Tibshirani (2000). ``Additive Logistic\n",
       "Regression: a Statistical View of Boosting,'' \\emph{Annals of Statistics}\n",
       "28(2):337-374.\n",
       "\n",
       "J.H. Friedman (2001). ``Greedy Function Approximation: A Gradient\n",
       "Boosting Machine,'' \\emph{Annals of Statistics} 29(5):1189-1232.\n",
       "\n",
       "J.H. Friedman (2002). ``Stochastic Gradient Boosting,''\n",
       "\\emph{Computational Statistics and Data Analysis} 38(4):367-378.\n",
       "\n",
       "B. Kriegler (2007). Cost-Sensitive Stochastic Gradient Boosting Within a \n",
       "Quantitative Regression Framework. Ph.D. Dissertation. University of \n",
       "California at Los Angeles, Los Angeles, CA, USA. Advisor(s) Richard A. Berk. \n",
       "urlhttps://dl.acm.org/citation.cfm?id=1354603.\n",
       "\n",
       "C. Burges (2010). ``From RankNet to LambdaRank to LambdaMART: An\n",
       "Overview,'' Microsoft Research Technical Report MSR-TR-2010-82.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{\\LinkA{gbm.object}{gbm.object}}, \\code{\\LinkA{gbm.perf}{gbm.perf}}, \n",
       "\\code{\\LinkA{plot.gbm}{plot.gbm}}, \\code{\\LinkA{predict.gbm}{predict.gbm}}, \\code{\\LinkA{summary.gbm}{summary.gbm}}, \n",
       "and \\code{\\LinkA{pretty.gbm.tree}{pretty.gbm.tree}}.\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "#\n",
       "# A least squares regression example \n",
       "#\n",
       "\n",
       "# Simulate data\n",
       "set.seed(101)  # for reproducibility\n",
       "N <- 1000\n",
       "X1 <- runif(N)\n",
       "X2 <- 2 * runif(N)\n",
       "X3 <- ordered(sample(letters[1:4], N, replace = TRUE), levels = letters[4:1])\n",
       "X4 <- factor(sample(letters[1:6], N, replace = TRUE))\n",
       "X5 <- factor(sample(letters[1:3], N, replace = TRUE))\n",
       "X6 <- 3 * runif(N) \n",
       "mu <- c(-1, 0, 1, 2)[as.numeric(X3)]\n",
       "SNR <- 10  # signal-to-noise ratio\n",
       "Y <- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu\n",
       "sigma <- sqrt(var(Y) / SNR)\n",
       "Y <- Y + rnorm(N, 0, sigma)\n",
       "X1[sample(1:N,size=500)] <- NA  # introduce some missing values\n",
       "X4[sample(1:N,size=300)] <- NA  # introduce some missing values\n",
       "data <- data.frame(Y, X1, X2, X3, X4, X5, X6)\n",
       "\n",
       "# Fit a GBM\n",
       "set.seed(102)  # for reproducibility\n",
       "gbm1 <- gbm(Y ~ ., data = data, var.monotone = c(0, 0, 0, 0, 0, 0),\n",
       "            distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1,             \n",
       "            interaction.depth = 3, bag.fraction = 0.5, train.fraction = 0.5,  \n",
       "            n.minobsinnode = 10, cv.folds = 5, keep.data = TRUE, \n",
       "            verbose = FALSE, n.cores = 1)  \n",
       "\n",
       "# Check performance using the out-of-bag (OOB) error; the OOB error typically\n",
       "# underestimates the optimal number of iterations\n",
       "best.iter <- gbm.perf(gbm1, method = \"OOB\")\n",
       "print(best.iter)\n",
       "\n",
       "# Check performance using the 50% heldout test set\n",
       "best.iter <- gbm.perf(gbm1, method = \"test\")\n",
       "print(best.iter)\n",
       "\n",
       "# Check performance using 5-fold cross-validation\n",
       "best.iter <- gbm.perf(gbm1, method = \"cv\")\n",
       "print(best.iter)\n",
       "\n",
       "# Plot relative influence of each variable\n",
       "par(mfrow = c(1, 2))\n",
       "summary(gbm1, n.trees = 1)          # using first tree\n",
       "summary(gbm1, n.trees = best.iter)  # using estimated best number of trees\n",
       "\n",
       "# Compactly print the first and last trees for curiosity\n",
       "print(pretty.gbm.tree(gbm1, i.tree = 1))\n",
       "print(pretty.gbm.tree(gbm1, i.tree = gbm1$n.trees))\n",
       "\n",
       "# Simulate new data\n",
       "set.seed(103)  # for reproducibility\n",
       "N <- 1000\n",
       "X1 <- runif(N)\n",
       "X2 <- 2 * runif(N)\n",
       "X3 <- ordered(sample(letters[1:4], N, replace = TRUE))\n",
       "X4 <- factor(sample(letters[1:6], N, replace = TRUE))\n",
       "X5 <- factor(sample(letters[1:3], N, replace = TRUE))\n",
       "X6 <- 3 * runif(N) \n",
       "mu <- c(-1, 0, 1, 2)[as.numeric(X3)]\n",
       "Y <- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu + rnorm(N, 0, sigma)\n",
       "data2 <- data.frame(Y, X1, X2, X3, X4, X5, X6)\n",
       "\n",
       "# Predict on the new data using the \"best\" number of trees; by default,\n",
       "# predictions will be on the link scale\n",
       "Yhat <- predict(gbm1, newdata = data2, n.trees = best.iter, type = \"link\")\n",
       "\n",
       "# least squares error\n",
       "print(sum((data2$Y - Yhat)^2))\n",
       "\n",
       "# Construct univariate partial dependence plots\n",
       "plot(gbm1, i.var = 1, n.trees = best.iter)\n",
       "plot(gbm1, i.var = 2, n.trees = best.iter)\n",
       "plot(gbm1, i.var = \"X3\", n.trees = best.iter)  # can use index or name\n",
       "\n",
       "# Construct bivariate partial dependence plots\n",
       "plot(gbm1, i.var = 1:2, n.trees = best.iter)\n",
       "plot(gbm1, i.var = c(\"X2\", \"X3\"), n.trees = best.iter)\n",
       "plot(gbm1, i.var = 3:4, n.trees = best.iter)\n",
       "\n",
       "# Construct trivariate partial dependence plots\n",
       "plot(gbm1, i.var = c(1, 2, 6), n.trees = best.iter, \n",
       "     continuous.resolution = 20)\n",
       "plot(gbm1, i.var = 1:3, n.trees = best.iter)\n",
       "plot(gbm1, i.var = 2:4, n.trees = best.iter)\n",
       "plot(gbm1, i.var = 3:5, n.trees = best.iter)\n",
       "\n",
       "# Add more (i.e., 100) boosting iterations to the ensemble\n",
       "gbm2 <- gbm.more(gbm1, n.new.trees = 100, verbose = FALSE)\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "gbm                    package:gbm                     R Documentation\n",
       "\n",
       "_\bG_\be_\bn_\be_\br_\ba_\bl_\bi_\bz_\be_\bd _\bB_\bo_\bo_\bs_\bt_\be_\bd _\bR_\be_\bg_\br_\be_\bs_\bs_\bi_\bo_\bn _\bM_\bo_\bd_\be_\bl_\bi_\bn_\bg (_\bG_\bB_\bM)\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     Fits generalized boosted regression models. For technical details,\n",
       "     see the vignette: 'utils::browseVignettes(\"gbm\")'.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     gbm(\n",
       "       formula = formula(data),\n",
       "       distribution = \"bernoulli\",\n",
       "       data = list(),\n",
       "       weights,\n",
       "       var.monotone = NULL,\n",
       "       n.trees = 100,\n",
       "       interaction.depth = 1,\n",
       "       n.minobsinnode = 10,\n",
       "       shrinkage = 0.1,\n",
       "       bag.fraction = 0.5,\n",
       "       train.fraction = 1,\n",
       "       cv.folds = 0,\n",
       "       keep.data = TRUE,\n",
       "       verbose = FALSE,\n",
       "       class.stratify.cv = NULL,\n",
       "       n.cores = NULL\n",
       "     )\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       " formula: A symbolic description of the model to be fit. The formula\n",
       "          may include an offset term (e.g. y~offset(n)+x). If\n",
       "          'keep.data = FALSE' in the initial call to 'gbm' then it is\n",
       "          the user's responsibility to resupply the offset to\n",
       "          'gbm.more'.\n",
       "\n",
       "distribution: Either a character string specifying the name of the\n",
       "          distribution to use or a list with a component 'name'\n",
       "          specifying the distribution and any additional parameters\n",
       "          needed. If not specified, 'gbm' will try to guess: if the\n",
       "          response has only 2 unique values, bernoulli is assumed;\n",
       "          otherwise, if the response is a factor, multinomial is\n",
       "          assumed; otherwise, if the response has class '\"Surv\"', coxph\n",
       "          is assumed; otherwise, gaussian is assumed.\n",
       "\n",
       "          Currently available options are '\"gaussian\"' (squared error),\n",
       "          '\"laplace\"' (absolute loss), '\"tdist\"' (t-distribution loss),\n",
       "          '\"bernoulli\"' (logistic regression for 0-1 outcomes),\n",
       "          '\"huberized\"' (huberized hinge loss for 0-1 outcomes),\n",
       "          classes), '\"adaboost\"' (the AdaBoost exponential loss for 0-1\n",
       "          outcomes), '\"poisson\"' (count outcomes), '\"coxph\"' (right\n",
       "          censored observations), '\"quantile\"', or '\"pairwise\"'\n",
       "          (ranking measure using the LambdaMart algorithm).\n",
       "\n",
       "          If quantile regression is specified, 'distribution' must be a\n",
       "          list of the form 'list(name = \"quantile\", alpha = 0.25)'\n",
       "          where 'alpha' is the quantile to estimate. The current\n",
       "          version's quantile regression method does not handle\n",
       "          non-constant weights and will stop.\n",
       "\n",
       "          If '\"tdist\"' is specified, the default degrees of freedom is\n",
       "          4 and this can be controlled by specifying 'distribution =\n",
       "          list(name = \"tdist\", df = DF)' where 'DF' is your chosen\n",
       "          degrees of freedom.\n",
       "\n",
       "          If \"pairwise\" regression is specified, 'distribution' must be\n",
       "          a list of the form\n",
       "          'list(name=\"pairwise\",group=...,metric=...,max.rank=...)'\n",
       "          ('metric' and 'max.rank' are optional, see below). 'group' is\n",
       "          a character vector with the column names of 'data' that\n",
       "          jointly indicate the group an instance belongs to (typically\n",
       "          a query in Information Retrieval applications). For training,\n",
       "          only pairs of instances from the same group and with\n",
       "          different target labels can be considered. 'metric' is the IR\n",
       "          measure to use, one of\n",
       "\n",
       "          list(\"conc\") Fraction of concordant pairs; for binary labels,\n",
       "              this is equivalent to the Area under the ROC Curve\n",
       "\n",
       "          : Fraction of concordant pairs; for binary labels, this is\n",
       "              equivalent to the Area under the ROC Curve\n",
       "\n",
       "          list(\"mrr\") Mean reciprocal rank of the highest-ranked\n",
       "              positive instance\n",
       "\n",
       "          : Mean reciprocal rank of the highest-ranked positive\n",
       "              instance\n",
       "\n",
       "          list(\"map\") Mean average precision, a generalization of 'mrr'\n",
       "              to multiple positive instances\n",
       "\n",
       "          : Mean average precision, a generalization of 'mrr' to\n",
       "              multiple positive instances\n",
       "\n",
       "          list(\"ndcg:\") Normalized discounted cumulative gain. The\n",
       "              score is the weighted sum (DCG) of the user-supplied\n",
       "              target values, weighted by log(rank+1), and normalized to\n",
       "              the maximum achievable value. This is the default if the\n",
       "              user did not specify a metric.\n",
       "\n",
       "          'ndcg' and 'conc' allow arbitrary target values, while binary\n",
       "          targets 0,1 are expected for 'map' and 'mrr'. For 'ndcg' and\n",
       "          'mrr', a cut-off can be chosen using a positive integer\n",
       "          parameter 'max.rank'. If left unspecified, all ranks are\n",
       "          taken into account.\n",
       "\n",
       "          Note that splitting of instances into training and validation\n",
       "          sets follows group boundaries and therefore only approximates\n",
       "          the specified 'train.fraction' ratio (the same applies to\n",
       "          cross-validation folds). Internally, queries are randomly\n",
       "          shuffled before training, to avoid bias.\n",
       "\n",
       "          Weights can be used in conjunction with pairwise metrics,\n",
       "          however it is assumed that they are constant for instances\n",
       "          from the same group.\n",
       "\n",
       "          For details and background on the algorithm, see e.g. Burges\n",
       "          (2010).\n",
       "\n",
       "    data: an optional data frame containing the variables in the model.\n",
       "          By default the variables are taken from\n",
       "          'environment(formula)', typically the environment from which\n",
       "          'gbm' is called. If 'keep.data=TRUE' in the initial call to\n",
       "          'gbm' then 'gbm' stores a copy with the object. If\n",
       "          'keep.data=FALSE' then subsequent calls to 'gbm.more' must\n",
       "          resupply the same dataset. It becomes the user's\n",
       "          responsibility to resupply the same data at this point.\n",
       "\n",
       " weights: an optional vector of weights to be used in the fitting\n",
       "          process. Must be positive but do not need to be normalized.\n",
       "          If 'keep.data=FALSE' in the initial call to 'gbm' then it is\n",
       "          the user's responsibility to resupply the weights to\n",
       "          'gbm.more'.\n",
       "\n",
       "var.monotone: an optional vector, the same length as the number of\n",
       "          predictors, indicating which variables have a monotone\n",
       "          increasing (+1), decreasing (-1), or arbitrary (0)\n",
       "          relationship with the outcome.\n",
       "\n",
       " n.trees: Integer specifying the total number of trees to fit. This is\n",
       "          equivalent to the number of iterations and the number of\n",
       "          basis functions in the additive expansion. Default is 100.\n",
       "\n",
       "interaction.depth: Integer specifying the maximum depth of each tree\n",
       "          (i.e., the highest level of variable interactions allowed). A\n",
       "          value of 1 implies an additive model, a value of 2 implies a\n",
       "          model with up to 2-way interactions, etc. Default is 1.\n",
       "\n",
       "n.minobsinnode: Integer specifying the minimum number of observations\n",
       "          in the terminal nodes of the trees. Note that this is the\n",
       "          actual number of observations, not the total weight.\n",
       "\n",
       "shrinkage: a shrinkage parameter applied to each tree in the expansion.\n",
       "          Also known as the learning rate or step-size reduction; 0.001\n",
       "          to 0.1 usually work, but a smaller learning rate typically\n",
       "          requires more trees. Default is 0.1.\n",
       "\n",
       "bag.fraction: the fraction of the training set observations randomly\n",
       "          selected to propose the next tree in the expansion. This\n",
       "          introduces randomnesses into the model fit. If 'bag.fraction'\n",
       "          < 1 then running the same model twice will result in similar\n",
       "          but different fits. 'gbm' uses the R random number generator\n",
       "          so 'set.seed' can ensure that the model can be reconstructed.\n",
       "          Preferably, the user can save the returned 'gbm.object' using\n",
       "          'save'. Default is 0.5.\n",
       "\n",
       "train.fraction: The first 'train.fraction * nrows(data)' observations\n",
       "          are used to fit the 'gbm' and the remainder are used for\n",
       "          computing out-of-sample estimates of the loss function.\n",
       "\n",
       "cv.folds: Number of cross-validation folds to perform. If 'cv.folds'>1\n",
       "          then 'gbm', in addition to the usual fit, will perform a\n",
       "          cross-validation, calculate an estimate of generalization\n",
       "          error returned in 'cv.error'.\n",
       "\n",
       "keep.data: a logical variable indicating whether to keep the data and\n",
       "          an index of the data stored with the object. Keeping the data\n",
       "          and index makes subsequent calls to 'gbm.more' faster at the\n",
       "          cost of storing an extra copy of the dataset.\n",
       "\n",
       " verbose: Logical indicating whether or not to print out progress and\n",
       "          performance indicators ('TRUE'). If this option is left\n",
       "          unspecified for 'gbm.more', then it uses 'verbose' from\n",
       "          'object'. Default is 'FALSE'.\n",
       "\n",
       "class.stratify.cv: Logical indicating whether or not the\n",
       "          cross-validation should be stratified by class. Defaults to\n",
       "          'TRUE' for 'distribution = \"multinomial\"' and is only\n",
       "          implemented for '\"multinomial\"' and '\"bernoulli\"'. The\n",
       "          purpose of stratifying the cross-validation is to help\n",
       "          avoiding situations in which training sets do not contain all\n",
       "          classes.\n",
       "\n",
       " n.cores: The number of CPU cores to use. The cross-validation loop\n",
       "          will attempt to send different CV folds off to different\n",
       "          cores. If 'n.cores' is not specified by the user, it is\n",
       "          guessed using the 'detectCores' function in the 'parallel'\n",
       "          package. Note that the documentation for 'detectCores' makes\n",
       "          clear that it is not failsafe and could return a spurious\n",
       "          number of available cores.\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     'gbm.fit' provides the link between R and the C++ gbm engine.\n",
       "     'gbm' is a front-end to 'gbm.fit' that uses the familiar R\n",
       "     modeling formulas. However, 'model.frame' is very slow if there\n",
       "     are many predictor variables. For power-users with many variables\n",
       "     use 'gbm.fit'. For general practice 'gbm' is preferable.\n",
       "\n",
       "     This package implements the generalized boosted modeling\n",
       "     framework. Boosting is the process of iteratively adding basis\n",
       "     functions in a greedy fashion so that each additional basis\n",
       "     function further reduces the selected loss function. This\n",
       "     implementation closely follows Friedman's Gradient Boosting\n",
       "     Machine (Friedman, 2001).\n",
       "\n",
       "     In addition to many of the features documented in the Gradient\n",
       "     Boosting Machine, 'gbm' offers additional features including the\n",
       "     out-of-bag estimator for the optimal number of iterations, the\n",
       "     ability to store and manipulate the resulting 'gbm' object, and a\n",
       "     variety of other loss functions that had not previously had\n",
       "     associated boosting algorithms, including the Cox partial\n",
       "     likelihood for censored data, the poisson likelihood for count\n",
       "     outcomes, and a gradient boosting implementation to minimize the\n",
       "     AdaBoost exponential loss function.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     A 'gbm.object' object.\n",
       "\n",
       "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
       "\n",
       "     Greg Ridgeway <email: gregridgeway@gmail.com>\n",
       "\n",
       "     Quantile regression code developed by Brian Kriegler <email:\n",
       "     bk@stat.ucla.edu>\n",
       "\n",
       "     t-distribution, and multinomial code developed by Harry Southworth\n",
       "     and Daniel Edwards\n",
       "\n",
       "     Pairwise code developed by Stefan Schroedl <email:\n",
       "     schroedl@a9.com>\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Y. Freund and R.E. Schapire (1997) \"A decision-theoretic\n",
       "     generalization of on-line learning and an application to\n",
       "     boosting,\" _Journal of Computer and System Sciences,_\n",
       "     55(1):119-139.\n",
       "\n",
       "     G. Ridgeway (1999). \"The state of boosting,\" _Computing Science\n",
       "     and Statistics_ 31:172-181.\n",
       "\n",
       "     J.H. Friedman, T. Hastie, R. Tibshirani (2000). \"Additive Logistic\n",
       "     Regression: a Statistical View of Boosting,\" _Annals of\n",
       "     Statistics_ 28(2):337-374.\n",
       "\n",
       "     J.H. Friedman (2001). \"Greedy Function Approximation: A Gradient\n",
       "     Boosting Machine,\" _Annals of Statistics_ 29(5):1189-1232.\n",
       "\n",
       "     J.H. Friedman (2002). \"Stochastic Gradient Boosting,\"\n",
       "     _Computational Statistics and Data Analysis_ 38(4):367-378.\n",
       "\n",
       "     B. Kriegler (2007). Cost-Sensitive Stochastic Gradient Boosting\n",
       "     Within a Quantitative Regression Framework. Ph.D. Dissertation.\n",
       "     University of California at Los Angeles, Los Angeles, CA, USA.\n",
       "     Advisor(s) Richard A. Berk.\n",
       "     urlhttps://dl.acm.org/citation.cfm?id=1354603.\n",
       "\n",
       "     C. Burges (2010). \"From RankNet to LambdaRank to LambdaMART: An\n",
       "     Overview,\" Microsoft Research Technical Report MSR-TR-2010-82.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     'gbm.object', 'gbm.perf', 'plot.gbm', 'predict.gbm',\n",
       "     'summary.gbm', and 'pretty.gbm.tree'.\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     #\n",
       "     # A least squares regression example \n",
       "     #\n",
       "     \n",
       "     # Simulate data\n",
       "     set.seed(101)  # for reproducibility\n",
       "     N <- 1000\n",
       "     X1 <- runif(N)\n",
       "     X2 <- 2 * runif(N)\n",
       "     X3 <- ordered(sample(letters[1:4], N, replace = TRUE), levels = letters[4:1])\n",
       "     X4 <- factor(sample(letters[1:6], N, replace = TRUE))\n",
       "     X5 <- factor(sample(letters[1:3], N, replace = TRUE))\n",
       "     X6 <- 3 * runif(N) \n",
       "     mu <- c(-1, 0, 1, 2)[as.numeric(X3)]\n",
       "     SNR <- 10  # signal-to-noise ratio\n",
       "     Y <- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu\n",
       "     sigma <- sqrt(var(Y) / SNR)\n",
       "     Y <- Y + rnorm(N, 0, sigma)\n",
       "     X1[sample(1:N,size=500)] <- NA  # introduce some missing values\n",
       "     X4[sample(1:N,size=300)] <- NA  # introduce some missing values\n",
       "     data <- data.frame(Y, X1, X2, X3, X4, X5, X6)\n",
       "     \n",
       "     # Fit a GBM\n",
       "     set.seed(102)  # for reproducibility\n",
       "     gbm1 <- gbm(Y ~ ., data = data, var.monotone = c(0, 0, 0, 0, 0, 0),\n",
       "                 distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1,             \n",
       "                 interaction.depth = 3, bag.fraction = 0.5, train.fraction = 0.5,  \n",
       "                 n.minobsinnode = 10, cv.folds = 5, keep.data = TRUE, \n",
       "                 verbose = FALSE, n.cores = 1)  \n",
       "     \n",
       "     # Check performance using the out-of-bag (OOB) error; the OOB error typically\n",
       "     # underestimates the optimal number of iterations\n",
       "     best.iter <- gbm.perf(gbm1, method = \"OOB\")\n",
       "     print(best.iter)\n",
       "     \n",
       "     # Check performance using the 50% heldout test set\n",
       "     best.iter <- gbm.perf(gbm1, method = \"test\")\n",
       "     print(best.iter)\n",
       "     \n",
       "     # Check performance using 5-fold cross-validation\n",
       "     best.iter <- gbm.perf(gbm1, method = \"cv\")\n",
       "     print(best.iter)\n",
       "     \n",
       "     # Plot relative influence of each variable\n",
       "     par(mfrow = c(1, 2))\n",
       "     summary(gbm1, n.trees = 1)          # using first tree\n",
       "     summary(gbm1, n.trees = best.iter)  # using estimated best number of trees\n",
       "     \n",
       "     # Compactly print the first and last trees for curiosity\n",
       "     print(pretty.gbm.tree(gbm1, i.tree = 1))\n",
       "     print(pretty.gbm.tree(gbm1, i.tree = gbm1$n.trees))\n",
       "     \n",
       "     # Simulate new data\n",
       "     set.seed(103)  # for reproducibility\n",
       "     N <- 1000\n",
       "     X1 <- runif(N)\n",
       "     X2 <- 2 * runif(N)\n",
       "     X3 <- ordered(sample(letters[1:4], N, replace = TRUE))\n",
       "     X4 <- factor(sample(letters[1:6], N, replace = TRUE))\n",
       "     X5 <- factor(sample(letters[1:3], N, replace = TRUE))\n",
       "     X6 <- 3 * runif(N) \n",
       "     mu <- c(-1, 0, 1, 2)[as.numeric(X3)]\n",
       "     Y <- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu + rnorm(N, 0, sigma)\n",
       "     data2 <- data.frame(Y, X1, X2, X3, X4, X5, X6)\n",
       "     \n",
       "     # Predict on the new data using the \"best\" number of trees; by default,\n",
       "     # predictions will be on the link scale\n",
       "     Yhat <- predict(gbm1, newdata = data2, n.trees = best.iter, type = \"link\")\n",
       "     \n",
       "     # least squares error\n",
       "     print(sum((data2$Y - Yhat)^2))\n",
       "     \n",
       "     # Construct univariate partial dependence plots\n",
       "     plot(gbm1, i.var = 1, n.trees = best.iter)\n",
       "     plot(gbm1, i.var = 2, n.trees = best.iter)\n",
       "     plot(gbm1, i.var = \"X3\", n.trees = best.iter)  # can use index or name\n",
       "     \n",
       "     # Construct bivariate partial dependence plots\n",
       "     plot(gbm1, i.var = 1:2, n.trees = best.iter)\n",
       "     plot(gbm1, i.var = c(\"X2\", \"X3\"), n.trees = best.iter)\n",
       "     plot(gbm1, i.var = 3:4, n.trees = best.iter)\n",
       "     \n",
       "     # Construct trivariate partial dependence plots\n",
       "     plot(gbm1, i.var = c(1, 2, 6), n.trees = best.iter, \n",
       "          continuous.resolution = 20)\n",
       "     plot(gbm1, i.var = 1:3, n.trees = best.iter)\n",
       "     plot(gbm1, i.var = 2:4, n.trees = best.iter)\n",
       "     plot(gbm1, i.var = 3:5, n.trees = best.iter)\n",
       "     \n",
       "     # Add more (i.e., 100) boosting iterations to the ensemble\n",
       "     gbm2 <- gbm.more(gbm1, n.new.trees = 100, verbose = FALSE)\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>4601</li><li>58</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4601\n",
       "\\item 58\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4601\n",
       "2. 58\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 4601   58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(spambase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ss = sample(1:nrow(spambase),floor(nrow(spambase)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>1138</li><li>2748</li><li>3203</li><li>3524</li><li>1416</li><li>2714</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1138\n",
       "\\item 2748\n",
       "\\item 3203\n",
       "\\item 3524\n",
       "\\item 1416\n",
       "\\item 2714\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1138\n",
       "2. 2748\n",
       "3. 3203\n",
       "4. 3524\n",
       "5. 1416\n",
       "6. 2714\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1138 2748 3203 3524 1416 2714"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(train_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spambase[train_ss,]\n",
    "validate = spambase[-train_ss,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>2300</li><li>58</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2300\n",
       "\\item 58\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2300\n",
       "2. 58\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2300   58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>2301</li><li>58</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2301\n",
       "\\item 58\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2301\n",
       "2. 58\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2301   58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(train)\n",
    "dim(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.2694             nan     0.1000    0.0308\n",
      "     2        1.2110             nan     0.1000    0.0284\n",
      "     3        1.1640             nan     0.1000    0.0231\n",
      "     4        1.1214             nan     0.1000    0.0211\n",
      "     5        1.0854             nan     0.1000    0.0159\n",
      "     6        1.0509             nan     0.1000    0.0172\n",
      "     7        1.0187             nan     0.1000    0.0156\n",
      "     8        0.9885             nan     0.1000    0.0135\n",
      "     9        0.9634             nan     0.1000    0.0118\n",
      "    10        0.9381             nan     0.1000    0.0116\n",
      "    20        0.7589             nan     0.1000    0.0066\n",
      "    40        0.5759             nan     0.1000    0.0036\n",
      "    60        0.4825             nan     0.1000    0.0016\n",
      "    80        0.4270             nan     0.1000    0.0006\n",
      "   100        0.3873             nan     0.1000    0.0001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod = gbm(is.spam~.,data=train,distribution=\"bernoulli\",n.trees=100,\n",
    "          shrinkage=0.1,interaction.depth=1,verbose=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 57  2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>var</th><th scope=col>rel.inf</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>char.freq.exclaim</th><td>char.freq.exclaim         </td><td>24.57910123</td></tr>\n",
       "\t<tr><th scope=row>char.freq.dollar</th><td>char.freq.dollar          </td><td>19.65930094</td></tr>\n",
       "\t<tr><th scope=row>word.freq.remove</th><td>word.freq.remove          </td><td>11.21820939</td></tr>\n",
       "\t<tr><th scope=row>word.freq.free</th><td>word.freq.free            </td><td> 8.17115692</td></tr>\n",
       "\t<tr><th scope=row>capital.run.length.average</th><td>capital.run.length.average</td><td> 8.00738204</td></tr>\n",
       "\t<tr><th scope=row>word.freq.hp</th><td>word.freq.hp              </td><td> 7.12628759</td></tr>\n",
       "\t<tr><th scope=row>word.freq.your</th><td>word.freq.your            </td><td> 6.91166911</td></tr>\n",
       "\t<tr><th scope=row>word.freq.money</th><td>word.freq.money           </td><td> 3.36581013</td></tr>\n",
       "\t<tr><th scope=row>capital.run.length.longest</th><td>capital.run.length.longest</td><td> 2.52208468</td></tr>\n",
       "\t<tr><th scope=row>word.freq.george</th><td>word.freq.george          </td><td> 2.21787604</td></tr>\n",
       "\t<tr><th scope=row>word.freq.our</th><td>word.freq.our             </td><td> 1.93685993</td></tr>\n",
       "\t<tr><th scope=row>word.freq.edu</th><td>word.freq.edu             </td><td> 1.42834123</td></tr>\n",
       "\t<tr><th scope=row>word.freq.internet</th><td>word.freq.internet        </td><td> 0.94069725</td></tr>\n",
       "\t<tr><th scope=row>word.freq.1999</th><td>word.freq.1999            </td><td> 0.50908256</td></tr>\n",
       "\t<tr><th scope=row>word.freq.000</th><td>word.freq.000             </td><td> 0.37195815</td></tr>\n",
       "\t<tr><th scope=row>word.freq.business</th><td>word.freq.business        </td><td> 0.21237589</td></tr>\n",
       "\t<tr><th scope=row>capital.run.length.total</th><td>capital.run.length.total  </td><td> 0.19788995</td></tr>\n",
       "\t<tr><th scope=row>word.freq.over</th><td>word.freq.over            </td><td> 0.16167357</td></tr>\n",
       "\t<tr><th scope=row>word.freq.re</th><td>word.freq.re              </td><td> 0.14365912</td></tr>\n",
       "\t<tr><th scope=row>word.freq.meeting</th><td>word.freq.meeting         </td><td> 0.11958347</td></tr>\n",
       "\t<tr><th scope=row>word.freq.will</th><td>word.freq.will            </td><td> 0.10249320</td></tr>\n",
       "\t<tr><th scope=row>word.freq.you</th><td>word.freq.you             </td><td> 0.09650759</td></tr>\n",
       "\t<tr><th scope=row>word.freq.make</th><td>word.freq.make            </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.address</th><td>word.freq.address         </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.all</th><td>word.freq.all             </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.3d</th><td>word.freq.3d              </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.order</th><td>word.freq.order           </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.mail</th><td>word.freq.mail            </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.receive</th><td>word.freq.receive         </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.people</th><td>word.freq.people          </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.report</th><td>word.freq.report          </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.addresses</th><td>word.freq.addresses       </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.email</th><td>word.freq.email           </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.credit</th><td>word.freq.credit          </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.font</th><td>word.freq.font            </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.hpl</th><td>word.freq.hpl             </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.650</th><td>word.freq.650             </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.lab</th><td>word.freq.lab             </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.labs</th><td>word.freq.labs            </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.telnet</th><td>word.freq.telnet          </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.857</th><td>word.freq.857             </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.data</th><td>word.freq.data            </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.415</th><td>word.freq.415             </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.85</th><td>word.freq.85              </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.technology</th><td>word.freq.technology      </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.parts</th><td>word.freq.parts           </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.pm</th><td>word.freq.pm              </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.direct</th><td>word.freq.direct          </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.cs</th><td>word.freq.cs              </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.original</th><td>word.freq.original        </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.project</th><td>word.freq.project         </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.table</th><td>word.freq.table           </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>word.freq.conference</th><td>word.freq.conference      </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>char.freq.semic</th><td>char.freq.semic           </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>char.freq.paren</th><td>char.freq.paren           </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>char.freq.bracket</th><td>char.freq.bracket         </td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>char.freq.pound</th><td>char.freq.pound           </td><td> 0.00000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 57  2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & var & rel.inf\\\\\n",
       "  & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\tchar.freq.exclaim & char.freq.exclaim          & 24.57910123\\\\\n",
       "\tchar.freq.dollar & char.freq.dollar           & 19.65930094\\\\\n",
       "\tword.freq.remove & word.freq.remove           & 11.21820939\\\\\n",
       "\tword.freq.free & word.freq.free             &  8.17115692\\\\\n",
       "\tcapital.run.length.average & capital.run.length.average &  8.00738204\\\\\n",
       "\tword.freq.hp & word.freq.hp               &  7.12628759\\\\\n",
       "\tword.freq.your & word.freq.your             &  6.91166911\\\\\n",
       "\tword.freq.money & word.freq.money            &  3.36581013\\\\\n",
       "\tcapital.run.length.longest & capital.run.length.longest &  2.52208468\\\\\n",
       "\tword.freq.george & word.freq.george           &  2.21787604\\\\\n",
       "\tword.freq.our & word.freq.our              &  1.93685993\\\\\n",
       "\tword.freq.edu & word.freq.edu              &  1.42834123\\\\\n",
       "\tword.freq.internet & word.freq.internet         &  0.94069725\\\\\n",
       "\tword.freq.1999 & word.freq.1999             &  0.50908256\\\\\n",
       "\tword.freq.000 & word.freq.000              &  0.37195815\\\\\n",
       "\tword.freq.business & word.freq.business         &  0.21237589\\\\\n",
       "\tcapital.run.length.total & capital.run.length.total   &  0.19788995\\\\\n",
       "\tword.freq.over & word.freq.over             &  0.16167357\\\\\n",
       "\tword.freq.re & word.freq.re               &  0.14365912\\\\\n",
       "\tword.freq.meeting & word.freq.meeting          &  0.11958347\\\\\n",
       "\tword.freq.will & word.freq.will             &  0.10249320\\\\\n",
       "\tword.freq.you & word.freq.you              &  0.09650759\\\\\n",
       "\tword.freq.make & word.freq.make             &  0.00000000\\\\\n",
       "\tword.freq.address & word.freq.address          &  0.00000000\\\\\n",
       "\tword.freq.all & word.freq.all              &  0.00000000\\\\\n",
       "\tword.freq.3d & word.freq.3d               &  0.00000000\\\\\n",
       "\tword.freq.order & word.freq.order            &  0.00000000\\\\\n",
       "\tword.freq.mail & word.freq.mail             &  0.00000000\\\\\n",
       "\tword.freq.receive & word.freq.receive          &  0.00000000\\\\\n",
       "\tword.freq.people & word.freq.people           &  0.00000000\\\\\n",
       "\tword.freq.report & word.freq.report           &  0.00000000\\\\\n",
       "\tword.freq.addresses & word.freq.addresses        &  0.00000000\\\\\n",
       "\tword.freq.email & word.freq.email            &  0.00000000\\\\\n",
       "\tword.freq.credit & word.freq.credit           &  0.00000000\\\\\n",
       "\tword.freq.font & word.freq.font             &  0.00000000\\\\\n",
       "\tword.freq.hpl & word.freq.hpl              &  0.00000000\\\\\n",
       "\tword.freq.650 & word.freq.650              &  0.00000000\\\\\n",
       "\tword.freq.lab & word.freq.lab              &  0.00000000\\\\\n",
       "\tword.freq.labs & word.freq.labs             &  0.00000000\\\\\n",
       "\tword.freq.telnet & word.freq.telnet           &  0.00000000\\\\\n",
       "\tword.freq.857 & word.freq.857              &  0.00000000\\\\\n",
       "\tword.freq.data & word.freq.data             &  0.00000000\\\\\n",
       "\tword.freq.415 & word.freq.415              &  0.00000000\\\\\n",
       "\tword.freq.85 & word.freq.85               &  0.00000000\\\\\n",
       "\tword.freq.technology & word.freq.technology       &  0.00000000\\\\\n",
       "\tword.freq.parts & word.freq.parts            &  0.00000000\\\\\n",
       "\tword.freq.pm & word.freq.pm               &  0.00000000\\\\\n",
       "\tword.freq.direct & word.freq.direct           &  0.00000000\\\\\n",
       "\tword.freq.cs & word.freq.cs               &  0.00000000\\\\\n",
       "\tword.freq.original & word.freq.original         &  0.00000000\\\\\n",
       "\tword.freq.project & word.freq.project          &  0.00000000\\\\\n",
       "\tword.freq.table & word.freq.table            &  0.00000000\\\\\n",
       "\tword.freq.conference & word.freq.conference       &  0.00000000\\\\\n",
       "\tchar.freq.semic & char.freq.semic            &  0.00000000\\\\\n",
       "\tchar.freq.paren & char.freq.paren            &  0.00000000\\\\\n",
       "\tchar.freq.bracket & char.freq.bracket          &  0.00000000\\\\\n",
       "\tchar.freq.pound & char.freq.pound            &  0.00000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 57  2\n",
       "\n",
       "| <!--/--> | var &lt;chr&gt; | rel.inf &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| char.freq.exclaim | char.freq.exclaim          | 24.57910123 |\n",
       "| char.freq.dollar | char.freq.dollar           | 19.65930094 |\n",
       "| word.freq.remove | word.freq.remove           | 11.21820939 |\n",
       "| word.freq.free | word.freq.free             |  8.17115692 |\n",
       "| capital.run.length.average | capital.run.length.average |  8.00738204 |\n",
       "| word.freq.hp | word.freq.hp               |  7.12628759 |\n",
       "| word.freq.your | word.freq.your             |  6.91166911 |\n",
       "| word.freq.money | word.freq.money            |  3.36581013 |\n",
       "| capital.run.length.longest | capital.run.length.longest |  2.52208468 |\n",
       "| word.freq.george | word.freq.george           |  2.21787604 |\n",
       "| word.freq.our | word.freq.our              |  1.93685993 |\n",
       "| word.freq.edu | word.freq.edu              |  1.42834123 |\n",
       "| word.freq.internet | word.freq.internet         |  0.94069725 |\n",
       "| word.freq.1999 | word.freq.1999             |  0.50908256 |\n",
       "| word.freq.000 | word.freq.000              |  0.37195815 |\n",
       "| word.freq.business | word.freq.business         |  0.21237589 |\n",
       "| capital.run.length.total | capital.run.length.total   |  0.19788995 |\n",
       "| word.freq.over | word.freq.over             |  0.16167357 |\n",
       "| word.freq.re | word.freq.re               |  0.14365912 |\n",
       "| word.freq.meeting | word.freq.meeting          |  0.11958347 |\n",
       "| word.freq.will | word.freq.will             |  0.10249320 |\n",
       "| word.freq.you | word.freq.you              |  0.09650759 |\n",
       "| word.freq.make | word.freq.make             |  0.00000000 |\n",
       "| word.freq.address | word.freq.address          |  0.00000000 |\n",
       "| word.freq.all | word.freq.all              |  0.00000000 |\n",
       "| word.freq.3d | word.freq.3d               |  0.00000000 |\n",
       "| word.freq.order | word.freq.order            |  0.00000000 |\n",
       "| word.freq.mail | word.freq.mail             |  0.00000000 |\n",
       "| word.freq.receive | word.freq.receive          |  0.00000000 |\n",
       "| word.freq.people | word.freq.people           |  0.00000000 |\n",
       "| word.freq.report | word.freq.report           |  0.00000000 |\n",
       "| word.freq.addresses | word.freq.addresses        |  0.00000000 |\n",
       "| word.freq.email | word.freq.email            |  0.00000000 |\n",
       "| word.freq.credit | word.freq.credit           |  0.00000000 |\n",
       "| word.freq.font | word.freq.font             |  0.00000000 |\n",
       "| word.freq.hpl | word.freq.hpl              |  0.00000000 |\n",
       "| word.freq.650 | word.freq.650              |  0.00000000 |\n",
       "| word.freq.lab | word.freq.lab              |  0.00000000 |\n",
       "| word.freq.labs | word.freq.labs             |  0.00000000 |\n",
       "| word.freq.telnet | word.freq.telnet           |  0.00000000 |\n",
       "| word.freq.857 | word.freq.857              |  0.00000000 |\n",
       "| word.freq.data | word.freq.data             |  0.00000000 |\n",
       "| word.freq.415 | word.freq.415              |  0.00000000 |\n",
       "| word.freq.85 | word.freq.85               |  0.00000000 |\n",
       "| word.freq.technology | word.freq.technology       |  0.00000000 |\n",
       "| word.freq.parts | word.freq.parts            |  0.00000000 |\n",
       "| word.freq.pm | word.freq.pm               |  0.00000000 |\n",
       "| word.freq.direct | word.freq.direct           |  0.00000000 |\n",
       "| word.freq.cs | word.freq.cs               |  0.00000000 |\n",
       "| word.freq.original | word.freq.original         |  0.00000000 |\n",
       "| word.freq.project | word.freq.project          |  0.00000000 |\n",
       "| word.freq.table | word.freq.table            |  0.00000000 |\n",
       "| word.freq.conference | word.freq.conference       |  0.00000000 |\n",
       "| char.freq.semic | char.freq.semic            |  0.00000000 |\n",
       "| char.freq.paren | char.freq.paren            |  0.00000000 |\n",
       "| char.freq.bracket | char.freq.bracket          |  0.00000000 |\n",
       "| char.freq.pound | char.freq.pound            |  0.00000000 |\n",
       "\n"
      ],
      "text/plain": [
       "                           var                        rel.inf    \n",
       "char.freq.exclaim          char.freq.exclaim          24.57910123\n",
       "char.freq.dollar           char.freq.dollar           19.65930094\n",
       "word.freq.remove           word.freq.remove           11.21820939\n",
       "word.freq.free             word.freq.free              8.17115692\n",
       "capital.run.length.average capital.run.length.average  8.00738204\n",
       "word.freq.hp               word.freq.hp                7.12628759\n",
       "word.freq.your             word.freq.your              6.91166911\n",
       "word.freq.money            word.freq.money             3.36581013\n",
       "capital.run.length.longest capital.run.length.longest  2.52208468\n",
       "word.freq.george           word.freq.george            2.21787604\n",
       "word.freq.our              word.freq.our               1.93685993\n",
       "word.freq.edu              word.freq.edu               1.42834123\n",
       "word.freq.internet         word.freq.internet          0.94069725\n",
       "word.freq.1999             word.freq.1999              0.50908256\n",
       "word.freq.000              word.freq.000               0.37195815\n",
       "word.freq.business         word.freq.business          0.21237589\n",
       "capital.run.length.total   capital.run.length.total    0.19788995\n",
       "word.freq.over             word.freq.over              0.16167357\n",
       "word.freq.re               word.freq.re                0.14365912\n",
       "word.freq.meeting          word.freq.meeting           0.11958347\n",
       "word.freq.will             word.freq.will              0.10249320\n",
       "word.freq.you              word.freq.you               0.09650759\n",
       "word.freq.make             word.freq.make              0.00000000\n",
       "word.freq.address          word.freq.address           0.00000000\n",
       "word.freq.all              word.freq.all               0.00000000\n",
       "word.freq.3d               word.freq.3d                0.00000000\n",
       "word.freq.order            word.freq.order             0.00000000\n",
       "word.freq.mail             word.freq.mail              0.00000000\n",
       "word.freq.receive          word.freq.receive           0.00000000\n",
       "word.freq.people           word.freq.people            0.00000000\n",
       "word.freq.report           word.freq.report            0.00000000\n",
       "word.freq.addresses        word.freq.addresses         0.00000000\n",
       "word.freq.email            word.freq.email             0.00000000\n",
       "word.freq.credit           word.freq.credit            0.00000000\n",
       "word.freq.font             word.freq.font              0.00000000\n",
       "word.freq.hpl              word.freq.hpl               0.00000000\n",
       "word.freq.650              word.freq.650               0.00000000\n",
       "word.freq.lab              word.freq.lab               0.00000000\n",
       "word.freq.labs             word.freq.labs              0.00000000\n",
       "word.freq.telnet           word.freq.telnet            0.00000000\n",
       "word.freq.857              word.freq.857               0.00000000\n",
       "word.freq.data             word.freq.data              0.00000000\n",
       "word.freq.415              word.freq.415               0.00000000\n",
       "word.freq.85               word.freq.85                0.00000000\n",
       "word.freq.technology       word.freq.technology        0.00000000\n",
       "word.freq.parts            word.freq.parts             0.00000000\n",
       "word.freq.pm               word.freq.pm                0.00000000\n",
       "word.freq.direct           word.freq.direct            0.00000000\n",
       "word.freq.cs               word.freq.cs                0.00000000\n",
       "word.freq.original         word.freq.original          0.00000000\n",
       "word.freq.project          word.freq.project           0.00000000\n",
       "word.freq.table            word.freq.table             0.00000000\n",
       "word.freq.conference       word.freq.conference        0.00000000\n",
       "char.freq.semic            char.freq.semic             0.00000000\n",
       "char.freq.paren            char.freq.paren             0.00000000\n",
       "char.freq.bracket          char.freq.bracket           0.00000000\n",
       "char.freq.pound            char.freq.pound             0.00000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAclBMVEUAAAAAAP8ABf8ACf8A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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'integer'"
      ],
      "text/latex": [
       "'integer'"
      ],
      "text/markdown": [
       "'integer'"
      ],
      "text/plain": [
       "[1] \"integer\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(head(spambase$is.spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (predict(mod,n.tree=100,type=\"response\")>.5)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     \n",
       "preds    0    1\n",
       "    0 1381  100\n",
       "    1   35  784"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(preds,train$is.spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 6  100 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>1</th><th scope=col>2</th><th scope=col>3</th><th scope=col>4</th><th scope=col>5</th><th scope=col>6</th><th scope=col>7</th><th scope=col>8</th><th scope=col>9</th><th scope=col>10</th><th scope=col>...</th><th scope=col>91</th><th scope=col>92</th><th scope=col>93</th><th scope=col>94</th><th scope=col>95</th><th scope=col>96</th><th scope=col>97</th><th scope=col>98</th><th scope=col>99</th><th scope=col>100</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 6  100 of type dbl\n",
       "\\begin{tabular}{lllllllllllllllllllll}\n",
       " 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & ... & 91 & 92 & 93 & 94 & 95 & 96 & 97 & 98 & 99 & 100\\\\\n",
       "\\hline\n",
       "\t 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & ... & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\\\\n",
       "\t 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 6  100 of type dbl\n",
       "\n",
       "| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | ... | 91 | 92 | 93 | 94 | 95 | 96 | 97 | 98 | 99 | 100 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
       "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "     1 2 3 4 5 6 7 8 9 10 ... 91 92 93 94 95 96 97 98 99 100\n",
       "[1,] 0 0 0 0 0 0 0 0 0 0  ... 1  1  1  1  1  1  1  1  1  1  \n",
       "[2,] 0 0 0 0 0 0 0 0 0 0  ... 0  0  0  0  0  0  0  0  0  0  \n",
       "[3,] 0 0 0 0 0 0 0 0 0 0  ... 0  0  0  0  0  0  0  0  0  0  \n",
       "[4,] 0 0 0 0 0 0 0 0 0 0  ... 0  0  0  0  0  0  0  0  0  0  \n",
       "[5,] 0 0 0 0 0 0 0 0 0 0  ... 0  0  0  0  0  0  0  0  0  0  \n",
       "[6,] 0 0 0 0 0 0 0 0 0 0  ... 0  0  0  0  0  0  0  0  0  0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>2300</li><li>100</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2300\n",
       "\\item 100\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2300\n",
       "2. 100\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2300  100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nseq = 1:100\n",
    "pred_mtx = (predict(mod,n.tree=nseq,type=\"response\")>.5)*1\n",
    "head(pred_mtx)\n",
    "dim(pred_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = sapply(nseq,function(i)mean(pred_mtx[,i]!=train$is.spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>0.384347826086957</li><li>0.384347826086957</li><li>0.21695652173913</li><li>0.21695652173913</li><li>0.21695652173913</li><li>0.2</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.384347826086957\n",
       "\\item 0.384347826086957\n",
       "\\item 0.21695652173913\n",
       "\\item 0.21695652173913\n",
       "\\item 0.21695652173913\n",
       "\\item 0.2\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.384347826086957\n",
       "2. 0.384347826086957\n",
       "3. 0.21695652173913\n",
       "4. 0.21695652173913\n",
       "5. 0.21695652173913\n",
       "6. 0.2\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.3843478 0.3843478 0.2169565 0.2169565 0.2169565 0.2000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(nseq,err,type='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\n86bdgehBAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nseq = 1:10000\n",
    "mod = gbm(is.spam~.,data=train,distribution=\"bernoulli\",n.trees=max(nseq),\n",
    "          shrinkage=0.1,interaction.depth=1)\n",
    "train_pred_mtx = (predict(mod,n.tree=nseq,type=\"response\")>.5)*1\n",
    "train_err = sapply(nseq,function(i)mean(train_pred_mtx[,i]!=train$is.spam))\n",
    "test_pred_mtx = (predict(mod,validate,n.tree=nseq,type=\"response\")>.5)*1\n",
    "test_err = sapply(nseq,function(i)mean(test_pred_mtx[,i]!=validate$is.spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\n0saezgAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(nseq,train_err,type='b')           \n",
    "abline(v=which.min(train_err))\n",
    "points(nseq,test_err,type='b',col='red')\n",
    "abline(v=which.min(test_err),col='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\n",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nseq = 1:1000\n",
    "mod = gbm(is.spam~.,data=train,distribution=\"bernoulli\",n.trees=max(nseq),\n",
    "          shrinkage=.5,interaction.depth=5)\n",
    "train_pred_mtx = (predict(mod,n.tree=nseq,type=\"response\")>.5)*1\n",
    "train_err = sapply(nseq,function(i)mean(train_pred_mtx[,i]!=train$is.spam))\n",
    "test_pred_mtx = (predict(mod,validate,n.tree=nseq,type=\"response\")>.5)*1\n",
    "test_err = sapply(nseq,function(i)mean(test_pred_mtx[,i]!=validate$is.spam))\n",
    "plot(nseq,train_err,type='b')           \n",
    "abline(v=which.min(train_err))\n",
    "points(nseq,test_err,type='b',col='red')\n",
    "abline(v=which.min(test_err),col='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd,R"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
